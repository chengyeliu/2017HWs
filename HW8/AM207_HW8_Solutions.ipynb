{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APMTH 207: Advanced Scientific Computing: \n",
    "## Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "## Homework #8\n",
    "**Harvard University**<br>\n",
    "**Spring 2017**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "**Due Date: ** Friday, March 31st, 2017 at 11:59pm\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers as well as your iPython notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Application of Data Augmentation\n",
    "\n",
    "A plant nursery in Cambridge is exprimentally cross-breeding two types of hibiscus flowers: blue and pink. The goal is to create an exotic flower whose petals are pink with a ring of blue on each. \n",
    "\n",
    "There are four types of child plant that can result from this cross-breeding: \n",
    "\n",
    "  - Type 1: blue petals\n",
    "  - Type 2: pink petals \n",
    "  - Type 3: purple petals\n",
    "  - Type 4: pink petals with a blue ring on each (the desired effect). \n",
    "\n",
    "Out of 197 initial cross-breedings, the nursery obtained the following distribution over the four types of child plants: \n",
    "$$Y = (y_1, y_2, y_3, y_4) = (125, 18, 20, 34)$$\n",
    "where $y_i$ represents the number of child plants that are of type $i$.\n",
    "\n",
    "The nursery then consulted a famed Harvard plant geneticist, who informed them that the probability of obtaining each type of child plant in any single breeding experiment is as follows:\n",
    "$$ \\frac{\\theta+2}{4}, \\frac{1-\\theta}{4}, \\frac{1-\\theta}{4}, \\frac{\\theta}{4}.$$\n",
    "Unfortunately, the geneticist did not specify the quantity $\\theta$.\n",
    "\n",
    "Clearly, the nursery is interested in understanding how many cross-breeding they must perform, on average, in order to obtain a certain number of child plants with the exotic blue rings. To do this they must be able to compute $\\theta$. \n",
    "\n",
    "The owners of the nursery, being top students in AM207, decided to model the experiment in hopes of discovering $\\theta$ using the results from their 197 initial experiments. \n",
    "\n",
    "They chose to model the observed data using a multinomial model and thus calculated the likelihood to be:\n",
    "$$ p(y  \\vert  \\theta) \\propto (2+\\theta)^{y_1} (1-\\theta)^{y_2+y_3}  \\, \\theta^{y_4}\n",
    "$$\n",
    "\n",
    "Being good Bayesians, they also imposed a prior on $\\theta$, $\\rm{Beta}(a, b)$.\n",
    "\n",
    "Thus, the posterior is:\n",
    "$$ p(\\theta \\vert  Y) \\propto \\left( 2+\\theta \\right)^{y_1} (1-\\theta)^{y_2+y_3} \\, \\theta^{\n",
    "y_4} \\, \\theta^{a-1} \\, (1-\\theta)^{b-1}. $$\n",
    "\n",
    "If the nursery owners are able to sample from the posterior, they would be understand the distribution of $\\theta$ and make appropriate estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Sampling using data augmentation\n",
    "\n",
    "Realizing that it would be difficult to sample from the posterior directly and after being repeatedly frustrated by attempts of Metropolis-Hastings and Gibbs sampling for this model, the nursery owners decided to augment their model and hopefully obtain a friendlier looking distribution that allows for easy sampling.\n",
    "\n",
    "They augment the data with a new variable $z$ such that:\n",
    "$$z + (y_1 - z) = y_1.$$\n",
    "That is, using $z$, we are breaking $y_1$, the number of type I child plants, into two subtypes. Let the probability of obtain the two subtype be $1/2$ and $\\theta/4$, respectively. Now, we can interpret $y_1$ to be the total number of trials in a binomial trial. Thus, the new likelihood can be written as\n",
    "$$ p(y, z  \\vert  \\theta) \\propto \\binom{y_{1}}{z} \\left (\\frac{1}{2} \\right )^{y_1-z} \\left(\\frac{\\theta}{4} \\right )^{z}  (1-\\theta)^{y_2+y_3}  \\, \\theta^{y_4}\n",
    "$$\n",
    "\n",
    "\n",
    "Derived the joint posterior $p(\\theta, z  \\vert  y)$ and sample from it using Gibbs sampling.\n",
    "\n",
    "Visualize the distribution of theta and, from this distribution, estimate the probability of obtaining a type 4 child plant (with the blue rings) in any cross-breeding experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solutions for TFS**\n",
    "\n",
    "In the first instance, where you have four outcomes each with some probability, we're setting up the observed counts of each of the four outcomes (the $y_i$'s) as a multinomial trial, so then the likelihood is just the product of the probability of the first outcome raised to the power of how many times that outcome happend ($y_1$) etc...with some normalizing constants (all involving just $y_i$'s)\n",
    "$$ \n",
    "p(y  \\vert  \\theta) \\propto (2+\\theta)^{y_1} (1-\\theta)^{y_2+y_3}  \\, \\theta^{y_4}\n",
    "$$\n",
    "Now, since the normalizing constant contain only $y$'s we can just drop them (if we're only interested in sampling).\n",
    "\n",
    "Now, when we introduce a latent var $Z$, this is essentially breaking the first outcome into two separate outcomes. To put it simply, we now have ***five*** outcomes instead of four. We still set up the likelihood as a multinomial trial (now with five categories instead of four) and so the likelihood is still proprotional to the product of the probability of the first outcome raised to the power of how many times that outcome happend ($y_1$) etc.\n",
    "\n",
    "$$ p(y, z  \\vert  \\theta) \\propto \\binom{y_{1}}{z} \\left (\\frac{1}{2} \\right )^{y_1-z} \\left(\\frac{\\theta}{4} \\right )^{z}  (1-\\theta)^{y_2+y_3}  \\, \\theta^{y_4}\n",
    "$$\n",
    "\n",
    "So where did the $\\binom{y_{1}}{z}$ come from? Well, write out the normalizing constant for the new multinomial model with five categories, some parts of this constants will involve $z$ and $y_1$, since $z$ is not a constant (we'll be sampling from it later) we can't drop these parts of the normalizing constant. If you keep the parts of the normalizing constant involving $z$ (and a bit extra) then you'll get the coefficient $\\binom{y_{1}}{z}$.\n",
    "\n",
    "\n",
    "Ok, so to get the joint $$ p(y, z, \\theta)$$ we multiply the likelihood by the prior\n",
    "$$ p(y, z  \\vert  \\theta) \\propto \\binom{y_{1}}{z} \\left (\\frac{1}{2} \\right )^{y_1-z} \\left(\\frac{\\theta}{4} \\right )^{z}  (1-\\theta)^{y_2+y_3}  \\, \\theta^{y_4} \\theta^{a-1} (1-\\theta)^{b-1}\n",
    "$$\n",
    "\n",
    "To get each conditional, you just write down the factors from the joint that include the relevant variable!\n",
    "\n",
    "$$\n",
    "p(z | \\theta, y)\\propto \\binom{y_{1}}{z} \\left (\\frac{1}{2} \\right )^{y_1-z} \\left(\\frac{\\theta}{4} \\right )^{z} \n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "p(\\theta | z, y)\\propto (1-\\theta)^{y_2+y_3}  \\, \\theta^{y_4} \\theta^{a-1} (1-\\theta)^{b-1} = (1-\\theta)^{y_2+y_3 + b - 1}  \\, \\theta^{y_4 + a -1}\n",
    "$$\n",
    "\n",
    "So $p(\\theta | z, y)$ looks like a beta pdf! I.e. $\\theta | z, y \\sim Beta(y_4 + a -1, y_2+y_3 + b - 1)$.\n",
    "\n",
    "And $p(z | \\theta, y)$ looks like a binomial pdf except $\\frac{1}{2} + \\frac{\\theta}{4} = \\frac{2 + \\theta}{4}$ is not equal to 1 (in a binomial pdf, the bases of the exponential factors need to sum to 1). Ok, so we just multiply the expression by $\\frac{[4/(2 + \\theta)]^{(y_1 - z) + z}}{[4/(2 + \\theta)]^{y_1}}$ and get\n",
    "\n",
    "$$\n",
    "p(z | \\theta, y)\\propto \\binom{y_{1}}{z} \\left (\\frac{1}{2} \\right )^{y_1-z} \\left(\\frac{\\theta}{4} \\right )^{z} \\frac{[4/(2 + \\theta)]^{(y_1 - z) + z}}{[4/(2 + \\theta)]^{y_1}}= \\left(\\frac{2 + \\theta}{4}\\right)^{y_1} \\binom{y_{1}}{z} \\left (\\frac{2}{2 + \\theta} \\right )^{y_1-z} \\left(\\frac{\\theta}{2 + \\theta} \\right )^{z}\n",
    "$$\n",
    "\n",
    "But again $\\left(\\frac{2 + \\theta}{4}\\right)^{y_1}$ is constant with respect to $z$ so when we're sampling $z$ we can just drop this factor. That is, we just have\n",
    "\n",
    "$$\n",
    "p(z | \\theta, y)\\propto  \\binom{y_{1}}{z} \\left (\\frac{2}{2 + \\theta} \\right )^{y_1-z} \\left(\\frac{\\theta}{2 + \\theta} \\right )^{z}\n",
    "$$\n",
    "\n",
    "and that is a binomial distribution $Bin\\left(y_1, \\frac{\\theta}{2 + \\theta} \\right )$.\n",
    "\n",
    "\n",
    "So your Gibbs sampler:\n",
    "\n",
    "0. randomly initialize $\\theta$ and $z$\n",
    "1. sample $\\theta | z, y \\sim Beta(y_4 + a -1, y_2+y_3 + b - 1)$\n",
    "2. sample $z | \\theta, y \\sim Bin\\left(y_1, \\frac{\\theta}{2 + \\theta} \\right )$.\n",
    "\n",
    "After burn-in and thinning. Plot the samples of $\\theta$ (ignore the $z$'s - we don't care about them). \n",
    "\n",
    "Finally, make some point estimate from the samples of $\\theta$, i.e. compute the mean or mode.\n",
    "\n",
    "Use this estimate of $\\theta$ to get the probability of obtaining the desired outcome in the cross breeding experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
