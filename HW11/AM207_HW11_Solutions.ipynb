{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APMTH 207: Advanced Scientific Computing: \n",
    "## Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "## Homework #11\n",
    "**Harvard University**<br>\n",
    "**Spring 2017**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "**Due Date: ** Wednesday, April 26th 2017 at 11:59pm\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers as well as your iPython notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1: Homework #8 Revisited\n",
    "\n",
    "Recall the context of Homework #8: \n",
    "\n",
    "A plant nursery in Cambridge is exprimentally cross-breeding two types of hibiscus flowers: blue and pink. The goal is to create an exotic flower whose petals are pink with a ring of blue on each. \n",
    "\n",
    "There are four types of child plant that can result from this cross-breeding: \n",
    "\n",
    "  - Type 1: blue petals\n",
    "  - Type 2: pink petals \n",
    "  - Type 3: purple petals\n",
    "  - Type 4: pink petals with a blue ring on each (the desired effect). \n",
    "\n",
    "Out of 197 initial cross-breedings, the nursery obtained the following distribution over the four types of child plants: \n",
    "$$Y = (y_1, y_2, y_3, y_4) = (125, 18, 20, 34)$$\n",
    "where $y_i$ represents the number of child plants that are of type $i$.\n",
    "\n",
    "They know that the probability of obtaining each type of child plant in any single breeding experiment is as follows:\n",
    "$$ \\frac{\\theta+2}{4}, \\frac{1-\\theta}{4}, \\frac{1-\\theta}{4}, \\frac{\\theta}{4},$$\n",
    "where $\\theta$ is unknown.\n",
    "\n",
    "Sensibly, the nursery chose to model the observed data using a multinomial model; *they also imposed a prior on $\\theta$, $\\rm{Beta}(a, b)$*.\n",
    "\n",
    "Recall that to simplify sampling from their Bayesian model, the nursery augmented the data with a new variable $z$ such that:\n",
    "$$z + (y_1 - z) = y_1.$$\n",
    "That is, using $z$, they are breaking $y_1$, the number of type I child plants, into two subtypes. Let the probability of obtain the two subtype be $1/2$ and $\\theta/4$, respectively.\n",
    "\n",
    "In Homework 8, you implemented a Gibbs sampler for this Bayesian model to compute the posterior mean estimate of $\\theta$. \n",
    "\n",
    "In this homework we will investigate ways to compute the Maximum Likelihood Estimate (MLE) of $\\theta$.\n",
    "\n",
    "***Note:*** Expectation Maximization can also be applied to compute the posterior mode (MAP) estimates. We are choosing not to do that in this homework so that you are not just repeating the task from Homework #8.\n",
    "\n",
    "\n",
    "### Part A:\n",
    "\n",
    "Treat the augmented model as a latent variable model. Write down an expression (up to unimportant constants - you must decide what unimportant means) for each of the following:\n",
    "\n",
    "(1) the observed data log likelihood\n",
    "\n",
    "(2) the complete(full) data log likelihood\n",
    "\n",
    "**Hint:** You should already have the observed data likelihood and the complete data likelihood from Homework #8, you just need to take their logs for this problem.\n",
    "\n",
    "(3) the Auxilary function, $Q(\\theta, \\theta^{(t-1)})$, or the expected complete(full) data log likelihood, defined by\n",
    "$$Q(\\theta, \\theta^{(t-1)}) = \\mathbb{E}_{Z  \\vert  Y=y, \\Theta = \\theta^{t-1}}[\\text{the complete data log likelihood}]$$\n",
    "\n",
    "In other words $Z  \\vert  Y=y, \\Theta = \\theta^{t-1}$ is $q(z, \\theta_{old})$ at the end of the E-step from the EM lecture. The Auxilary function $Q$ is the ELBO minus the entropy of $q$ (which being evaluated at $\\theta_{old}$ is not dependent on $\\theta$ and thus irrelevant for maximization).\n",
    "\n",
    "### Part B:\n",
    "\n",
    "We will maximize the likelihood through Expectation Maximization (EM). In order to preform EM, we must iterate through the following steps\n",
    "\n",
    "- (Expectation) Compute the Auxilary function, $Q(\\theta, \\theta^{t-1})$ (the expectation of the full data likelihood)\n",
    "- (Maximization) Compute $\\theta^{t} = \\text{argmax}_\\theta Q(\\theta, \\theta^{(t-1)})$\n",
    "\n",
    "Thus, you must compute exact formulae for the following:\n",
    "1. the Auxilary function, $Q(\\theta, \\theta^{(t-1)})$, for a given $\\theta^{(t-1)}$. That is, compute the expectation of the complete data log likelihood.\n",
    "2. $\\theta^{t}$, by maximizing the Auxilary function $Q(\\theta, \\theta^{(t-1)})$.\n",
    "\n",
    "**Hint:** You don't actually need to do any difficult optimization for the M-step. After taking the expectation of the complete data log likelihood in the E-step, match your $Q(\\theta, \\theta^{(t-1)})$ to the log pdf of a familiar distribution, then use the known formula for the mode of this distribution to optimize $Q(\\theta, \\theta^{(t-1)})$.\n",
    "\n",
    "Use these to **estimate the MLE** of $\\theta$ using EM (choose your own reasonable criterion for convergence).\n",
    "\n",
    "### Extra Credit:\n",
    "\n",
    "Explain the advantage of treating this problem like a latent variable model and using EM to compute the MLE (i.e. why not compute MLE directly by maximizing the likelihood?)\n",
    "\n",
    "Compare this value with the posterior mean estimate of $\\theta$ from Homework #8. In general, what is the difference between MLE and MAP or posterior mean estimates of model parameters? That is, name a couple of major pro's and con's of each type of estimate.\n",
    "\n",
    "\n",
    "**Solution:**\n",
    "Model:\n",
    "\\begin{align}\n",
    "z | y, \\theta &\\sim Bin\\left(y_1, \\frac{\\theta}{2 + \\theta}\\right)\\\\\n",
    "y | \\theta & \\sim Mult\\left(197, \\left[\\frac{1}{2} + \\frac{\\theta}{4}, \\frac{1 - \\theta}{4}, \\frac{1 - \\theta}{4}, \\frac{\\theta}{4} \\right]\\right)\n",
    "\\end{align}\n",
    "\n",
    "Observed:\n",
    "$$\n",
    "p(y | \\theta) = Mult\\left(y; 197, \\left[\\frac{1}{2} + \\frac{\\theta}{4}, \\frac{1 - \\theta}{4}, \\frac{1 - \\theta}{4}, \\frac{\\theta}{4} \\right]\\right)\n",
    "$$\n",
    "Complete:\n",
    "$$\n",
    "p(y, z | \\theta) = Bin\\left(z; y_1, \\frac{\\theta}{2 + \\theta}\\right)Mult\\left(y; 197, \\left[\\frac{1}{2} + \\frac{\\theta}{4}, \\frac{1 - \\theta}{4}, \\frac{1 - \\theta}{4}, \\frac{\\theta}{4} \\right]\\right)\n",
    "$$\n",
    "\n",
    "Auxilary function \n",
    "$$\n",
    "Q(\\theta, \\theta^{t-1}) =& \\mathbb{E}_{z | y, \\theta}\\left[\\log p(y, z | \\theta) \\right] \n",
    "$$\n",
    "\n",
    "For the E-step we're going to drop terms not involving $z$ or $\\theta$ since these will be constant in both the E and the M steps:\n",
    "\\begin{align}\n",
    "Q(\\theta, \\theta^{t-1}) =& \\mathbb{E}_{z | y, \\theta}\\left[\\log p(y, z | \\theta) \\right] \\\\\n",
    "\\propto& \\mathbb{E}_{z | y, \\theta}\\left[ \\log \\left\\{\\binom{y_1}{z} \\left( \\frac{1}{2}\\right)^{y_1 - z} \\left( \\frac{\\theta}{4}\\right)^{z} (1 - \\theta)^{y_2 + y_3} \\theta^{y_4}\\right\\}\\right]\\\\\n",
    "=& \\mathbb{E}_{z | y, \\theta}\\left[  \\log\\binom{y_1}{z} +  (y_1 - z)\\log\\frac{1}{2}  + z\\log \\frac{\\theta}{4} +  (y_2 + y_3) \\log (1 - \\theta) + y_4 \\log \\theta \\right]\\\\\n",
    "=&  \\mathbb{E}_{z | y, \\theta}\\left[\\log\\binom{y_1}{z}\\right] +   \\mathbb{E}_{z | y, \\theta} \\left[(y_1 - z)\\log\\frac{1}{2}\\right]  + \\mathbb{E}_{z | y, \\theta} \\left[z\\log \\theta\\right] \\\\\n",
    "& - \\mathbb{E}_{z | y, \\theta} \\log 4 +  \\mathbb{E}_{z | y, \\theta} \\left[(y_2 + y_3) \\log (1 - \\theta)\\right] + \\mathbb{E}_{z | y, \\theta}\\left[y_4 \\log \\theta\\right]\\\\\n",
    "=&  \\mathbb{E}_{z | y, \\theta}\\left[\\log\\binom{y_1}{z}\\right] +  \\log\\frac{1}{2}  \\left[\\mathbb{E}_{z | y, \\theta} y_1 - \\mathbb{E}_{z | y, \\theta} z\\right]  + \\log \\theta\\mathbb{E}_{z | y, \\theta} z\\\\\n",
    "& - \\mathbb{E}_{z | y, \\theta} \\log 4 +  \\mathbb{E}_{z | y, \\theta} \\left[(y_2 + y_3) \\log (1 - \\theta)\\right] + \\mathbb{E}_{z | y, \\theta}\\left[y_4 \\log \\theta\\right]\\\\\n",
    "=&  F_1(z, \\theta^{t-1}) +  \\log\\frac{1}{2} \\left(y_1 - \\frac{y_1\\theta^{t-1}}{2 + \\theta^{t-1}}\\right)  + \\log \\theta\\frac{y_1\\theta^{t-1}}{2 + \\theta^{t-1}} - \\log 4 +  (y_2 + y_3) \\log (1 - \\theta) + y_4 \\log \\theta\\\\\n",
    "=&  F_1(z, \\theta^{t-1}) +  F_2(z, \\theta^{t-1}) - \\log 4   +  (y_2 + y_3) \\log (1 - \\theta) + \\left(y_4  + \\frac{y_1\\theta^{t-1}}{2 + \\theta^{t-1}}\\right) \\theta\\\\\n",
    "\\end{align}\n",
    "\n",
    "For the M step we maximize the above with respect to $\\theta$, and thus can drop any terms not involving $\\theta$:\n",
    "\\begin{align}\n",
    "\\text{argmax}_\\theta \\left[ F_1(z, \\theta^{t-1}) +  F_2(z, \\theta^{t-1}) - \\log 4   +  (y_2 + y_3) \\log (1 - \\theta) + \\left(y_4  + \\frac{y_1\\theta^{t-1}}{2 + \\theta^{t-1}}\\right) \\theta\\right] = \\text{argmax}_\\theta\\left[(y_2 + y_3) \\log (1 - \\theta) + \\left(y_4  + \\frac{y_1\\theta^{t-1}}{2 + \\theta^{t-1}}\\right) \\theta\\right]\n",
    "\\end{align}\n",
    "But the above looks like (up to a constant) the log likelihood of a binomial pdf, i.e. \n",
    "\\begin{align}\n",
    "\\text{argmax}_\\theta\\left[(y_2 + y_3) \\log (1 - \\theta) + \\left(y_4  + \\frac{y_1\\theta^{t-1}}{2 + \\theta^{t-1}}\\right) \\theta\\right] =  \\text{argmax}_\\theta \\log Binom\\left(B; A + B, \\theta  \\right)+ C\n",
    "\\end{align}\n",
    "where $A = y_2 + y_3$ and $B = y_4  + \\frac{y_1\\theta^{t-1}}{2 + \\theta^{t-1}}$. So $\\argmax_\\theta$ of the above is just taking the MLE of a binomial. But the MLE of this binomial is \n",
    "$$\n",
    "\\frac{B}{A + B} = \\frac{ y_4  + \\frac{y_1\\theta^{t-1}}{2 + \\theta^{t-1}}}{ y_2 + y_3 + y_4  + \\frac{y_1\\theta^{t-1}}{2 + \\theta^{t-1}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.66568914956\n",
      "1 0.631838674952\n",
      "2 0.627485219761\n",
      "3 0.626909582965\n",
      "4 0.626833192939\n",
      "5 0.626823050714\n",
      "6 0.626821704055\n",
      "7 0.626821525248\n",
      "8 0.626821501506\n",
      "9 0.626821498354\n",
      "10 0.626821497935\n",
      "0.626821497935\n"
     ]
    }
   ],
   "source": [
    "conv_thresh = 1e-9\n",
    "iters = 1000\n",
    "diff = 1.\n",
    "y = (125,18,20,34)\n",
    "thetas = [1.]\n",
    "i = 0\n",
    "\n",
    "while i < iters and diff > conv_thresh:\n",
    "    theta = ((y[0] * thetas[-1])/(2 + thetas[-1]) + y[3])/((y[0] * thetas[-1])/(2 + thetas[-1]) + y[3] + y[2] + y[1])\n",
    "    print i, theta\n",
    "    i += 1\n",
    "    diff = abs(theta - thetas[-1])\n",
    "    thetas.append(theta)\n",
    "    \n",
    "print thetas[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
