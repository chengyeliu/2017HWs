{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APMTH 207: Advanced Scientific Computing: \n",
    "## Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "## Homework #3\n",
    "**Harvard University**<br>\n",
    "**Spring 2017**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "**Due Date: ** Thursday, Febrary 16th, 2017 at 11:59pm\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers as well as your iPython notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Optimization via Descent\n",
    "\n",
    "Suppose you are building a pricing model for laying down telecom cables over a geographical region. Your model takes as input a pair of  coordinates, $(x, y)$, and contains two parameters, $\\lambda_1, \\lambda_2$. Given a coordinate, $(x, y)$, and model parameters, the loss in revenue corresponding to the price model at location $(x, y)$ is described by\n",
    "$$\n",
    "L(x, y, \\lambda_1, \\lambda_2) = 0.00005\\lambda_2^2 y - 0.0001\\lambda_1^2 x -0.01\\lambda_1 x\\exp\\left\\{\\left(y^2 - x^2\\right)\\left(\\lambda_1^2 + \\lambda_2^2\\right)\\right\\}\n",
    "$$\n",
    "Read the data contained in `HW3_data.csv`. This is a set of coordinates configured on the curve $y^2 - x^2 = -0.1$. Given the data, find parameters $\\lambda_1, \\lambda_2$ that minimize the net loss over the entire dataset.\n",
    "\n",
    "### Part A\n",
    "- Visually verify that for $\\lambda_1 = -2.15784, \\lambda_2 = 0$, the loss function $L$ is minimized for the given data.\n",
    "- Implement gradient descent for minimizing $L$ for the given data, using the learning rate of 0.001.\n",
    "- Implement stochastic gradient descent for minimizing $L$ for the given data, using the learning rate of 0.001.\n",
    "\n",
    "### Part B\n",
    "- Compare the update time of the two implementations. Briefly explain whether your observations are what you expected.\n",
    "- Compare the number of iterations it takes for each algorithm to obtain an estimate accurate to `1e-3`. Briefly explain whether your observations are what you expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: More Varieties of Descent\n",
    "\n",
    "The following refers to the same problem set up in Problem 1.\n",
    "\n",
    "### Part A\n",
    "Compare the performance of stochastic gradient descent for the following learning rates: 1, 0.1, 0.001, 0.0001. Based on your observations, describe the effect of the choice of learning rate on the performance of the algorithm.\n",
    "\n",
    "### Part B\n",
    "Implement mini-batch gradient descent (for a few batch sizes). Compare the performance of mini-batch gradient descent for the following learning rates: 1, 0.1, 0.001, 0.0001. Based on your observations, compare the effect of learning rates on stochastic gradient descent and on mini-batch gradient descent."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
