{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APMTH 207: Advanced Scientific Computing: \n",
    "## Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "## Homework #5\n",
    "**Harvard University**<br>\n",
    "**Spring 2017**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "**Due Date: ** Thursday, March 2nd, 2017 at 11:59pm\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers as well as your iPython notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import collections\n",
    "import scipy as sp\n",
    "from scipy import stats \n",
    "from scipy.stats import multivariate_normal\n",
    "import time\n",
    "import math\n",
    "from functools import partial\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problem 1: Monte Carlo and Simulation Revisited\n",
    "In Homework #2, we used simulation to compute the expected values of functions of random variables. That is, given a random variable $X$, defined over $\\mathbb{R}$, distributed according to the pdf $f_X$, and given a real-valued function of $X$, $h(X)$, we approximated $\\mathbb{E}[h(X)]$ as follows\n",
    "$$\n",
    "\\mathbb{E}[h(X)] = \\int_{\\mathbb{R}} h(x)f_X(x) dx \\approx \\frac{1}{N} \\sum_{i=1}^N h(X_i), \\quad X_i \\sim f_X\n",
    "$$\n",
    "\n",
    "Now, suppose that, instead of being given the distribution $f_X$ and $h(X)$, you were simply asked to evaluate the following complex integral:\n",
    "$$\n",
    "I=\\int_{0}^{\\infty} \\frac{x^4\\, \\sin\\left(\\sqrt{\\ln{(x+1)}}\\right)e^{-x}}{2+(x-4)^2} \\, dx \n",
    "$$\n",
    "A clever way to apply our Monte Carlo techniques would be to split the integrand as $h(x)f_X(x)$, and then approximate the integral as we have done in Homework #2:\n",
    "$$\n",
    "I = \\int_{0}^{\\infty} h(x)\\,f_X(x) dx  \\approx \\frac{1}{N} \\sum\\limits_{i=1}^{N} h(X_i)$$ \n",
    "where the $X_i$'s are independently drawn from $f_X(x)$. \n",
    "\n",
    "We denote the approximation of the integral as follows\n",
    "$$\\hat{I} = \\frac{1}{N} \\sum\\limits_{i=1}^{N} h(X_i), \\quad X_i \\sim f_X.$$\n",
    "\n",
    "\n",
    "### Part A:\n",
    "\n",
    "Rewrite your integrand as a product of two functions, $h(x)g(x)$, which can then be expressed as $h(x)f_X(x)$, where $f_X$ is a pdf (you may use one of the splits we propose in Part B or create your own). Explain why your choice of $h$ is appropriate. Explain why your choice of $g$ is appropriate for creating a pdf $f_X$.\n",
    "\n",
    "(**Hint:** think about what you would have to do do turn $g$ into a good pdf and $h$ into a function that can be evaluated at multiple samples from this pdf. Think about how to choose these two functions to make your Monte Carlo approximation of $I$ as accurate as possible.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solutions:**\n",
    "\n",
    "After splitting, your integral will look like\n",
    "$$\n",
    "I = \\int_{0}^{\\infty} h(x) g(x) dx.\n",
    "$$\n",
    "This looks a lot like the setup for when we were computing the expected values of functions of random variables in Homework #2! That is, it looks like we're computing\n",
    "$$\n",
    "\\mathbb{E}_g[h(X)] = \\int_{0}^{\\infty} h(x) g(x) dx.\n",
    "$$\n",
    "But wait a second! The function $g$ is not necessarily a pdf and so the expression $\\mathbb{E}_g[h(X)]$ doesn't yet make sense. So let's turn $g$ into a pdf. For this we need to assume that $g$ is non-negative on $[0, \\infty)$ (since pdf's are non-negative funcitons). Let \n",
    "$$\n",
    "Z_g = \\int_{\\mathbb{R}} g(x) dx\n",
    "$$\n",
    "and define\n",
    "$$\n",
    "f_X = \\frac{g(x)}{Z_g}.\n",
    "$$\n",
    "Note that $f_X$ is a pdf on $[0, \\infty)$, since $f_X$ is non-negative on $[0, \\infty)$ and \n",
    "$$\n",
    "\\int_0^\\infty f_X(x) dx = \\int_0^\\infty \\frac{g(x)}{Z_g} dx  = \\frac{Z_g}{Z_g} = 1.\n",
    "$$\n",
    "Now we can rewrite our integral, $I$:\n",
    "$$\n",
    "I = Z_g \\int_{0}^{\\infty} h(x) \\frac{g(x)}{Z_g} dx = Z_g \\int_{0}^{\\infty} h(x) f_X(x) dx = \\mathbb{E}_{f_X}[Z_gh(X)] \n",
    "$$\n",
    "As in Homework #2, we can approximate the above by Monte Carlo simulation:\n",
    "$$\n",
    "I = \\mathbb{E}_{f_X}[Z_g h(X)] \\approx Z_g \\frac{1}{n}\\sum_{i=1}^n h(X_i), \\quad X_i \\sim f_X.\n",
    "$$\n",
    "\n",
    "**But what if $f_X$ is extremely complex and difficult to sample from?** In this case, we can sample from $f_X$ via the sampling methods in Homework #2, using a simpler proposal distribution, or we can produce samples from $f_X$ using MCMC. \n",
    "\n",
    "**So how should you split the integrand (which function is g and which is h)?** Generally, you want your choices to satisfy the following:\n",
    "0. $g$ must be non-negative where $h$ is non-zero - recall that pdf's must be non-negative functions.\n",
    "1. $g$ must be easy to integrate, since you need to integrate it to compute the normalizing constant $Z_g$.\n",
    "2. if possible, $g$ is easy to sample from. Otherwise, $g$ can be efficiently simulated (via rejection, importance, MCMC etc).\n",
    "3. if possible, $h$ is flat (small variance) over your $g$ since $Var[\\hat{I}] = \\frac{Var[h(X)]}{n}$\n",
    "4. furthermore, there are many additional factors to consider. For example, where should the probability mass of $g$ be, where $h$ has high variance or low? We encourage you to think about this and explore on your own, but we are not requiring this kind of depth in the analysis for this Homework.\n",
    "\n",
    "> **A Technical But Important Note:** Unless you can directly sample from $g$ and $g$ is naturally supported on $[0, \\infty)$, i.e. $g$ is naturally zero for negative values, you are better off re-defining your function $h$ and $g$ so that you can write your integral as\n",
    "$$\n",
    "I = \\int_{0}^{\\infty} {h}(x) {g}(x) dx = \\int_{-\\infty}^{\\infty} \\tilde{h}(x) \\tilde{g}(x) dx.\n",
    "$$\n",
    "The reason to do this is so that you don't need to worry about what to do with negative $x$-values you've sampled from $g$ or some proposal distribution whilst simulating $g$. Towards this end, we have two choices:\n",
    "\n",
    "> - leave $h(x)$ defined as it is and define a new function $\\tilde{g}$ as follows:\n",
    "$$\n",
    "\\tilde{g}(x) = \\begin{cases}\n",
    "g(x), & x \\geq 0\\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "Note that since $\\int_{-\\infty}^{\\infty} \\tilde{g}(x) dx = \\int_{0}^{\\infty} {g}(x) dx = Z_g$,  we can write our integral as:\n",
    "$$\n",
    "I = \\int_{0}^{\\infty} h(x) g(x) dx = \\int_{-\\infty}^{\\infty} h(x) \\tilde{g}(x) dx = Z_g \\int_{-\\infty}^{\\infty} h(x) \\frac{\\tilde{g}(x)}{Z_g} dx\n",
    "$$\n",
    "Thus, we can sample from $\\frac{\\tilde{g}(x)}{Z_g}$ or simulate sampling from $\\frac{\\tilde{g}(x)}{Z_g}$ (using MCMC) without taking any special care to treat negative $x$-values.\n",
    "\n",
    "> - if $g$ is a non-negative function on $\\mathbb{R}$, leave $g(x)$ defined as it is and define a new function $\\tilde{h}$ as follows:\n",
    "$$\n",
    "\\tilde{h}(x) = \\begin{cases}\n",
    "h(x), & x \\geq 0\\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "It is still the case that $I = \\int_{0}^{\\infty} h(x) g(x) dx = \\int_{-\\infty}^{\\infty} \\tilde{h}(x) g(x) dx$. But now, $g$ is supported on $\\mathbb{R}$ rather than $[0, \\infty)$ and we must recompute its normalizing constant on $\\mathbb{R}$:\n",
    "$$\n",
    "Z_{g_{\\mathbb{R}}} = \\int_{-\\infty}^{\\infty} g(x) dx.\n",
    "$$\n",
    "So we can express our integral as \n",
    "$$I = Z_{g_{\\mathbb{R}}}\\int_{-\\infty}^{\\infty} \\tilde{h}(x) \\frac{g(x)}{Z_{g_{\\mathbb{R}}}} dx,$$ \n",
    "and, again, we can sample from $\\frac{{g}(x)}{Z_{g_{\\mathbb{R}}}}$ or simulate sampling from it (using MCMC) without taking any special care to treat negative $x$-values.\n",
    "\n",
    "> ***WARNING:*** Do not do the following in order to deal with unwanted negative samples of $x$-values:\n",
    "  - sample from $g/Z_g$, supported over $\\mathbb{R}$, and then throw away all the negative samples. Now, the samples that are left are no longer from the distribution you've derived from $g$ and your estimate, $\\hat{I}$, is no longer valid (even if the answer looks correct).\n",
    "  \n",
    "  \n",
    "> - change the MH algorithm so that it automatically rejects negative proposals. In our special example, hard-coding a rejection in your MH algo is equivalent to defining $g$ to be zero on negative numbers, but hard-coding should be avoided as it can **esaily** correspond to changing the proposal distribution in unintended ways that violates detailed-balance or irreducibility. The point here is that when tweaking Metropolis Hastings, try to tweak the math (the function definitions) and not the algorithm (unless you are a sampling ninja and are willing to write proofs).\n",
    "\n",
    "### Part B:\n",
    "\n",
    "- Use $\\frac{1}{2+(x-4)^2}$ to create your pdf $f_X$. Implement a Metropolis algorithm to sample from $f_X$. Run the simulation 50 times for 150,000 points. Report the value of $\\hat{I}$ and that of Var[${\\hat{I}}$].\n",
    "\n",
    "\n",
    "- Use $xe^{-x}$ to create your pdf $f_X$. Implement a Metropolis algorithm to sample from $f_X$. Run the simulation 50 times for 150,000 points. Report the value of $\\hat{I}$ and that of Var[${\\hat{I}}$].\n",
    "\n",
    "\n",
    "- Compare the variance of your two estimates. Which choice of $f_X$ is better? Explain why.\n",
    "\n",
    "**Solution:** \n",
    "\n",
    "### Theoretical Variance of Monte Carlo Estimate for Each Split\n",
    "\n",
    "First, let's compute the theoretical values of $\\text{Var}[\\hat{I}]$ for the two choices of $g$'s. When possible, we recommend doing this step; knowing what kind of variance to expect will help us interpret unexpected results (i.e. did I code up something wrong or is this what I'm supposed to get).\n",
    "\n",
    "Recall that the variance in our Monte Carlo estimation $\\hat{I}$ is proportional to the variance of $h(X)$:\n",
    "\\begin{aligned}\n",
    "\\text{Var}[{\\hat{I}}] &= \\frac{\\text{Var}[h(X)]}{N}\\\\\n",
    "&= \\mathbb{E}[h(x)^2] -  \\mathbb{E}[h(x)]\\\\ \n",
    "&= \\frac{1}{N} \\left[Z_g \\int_{0}^{\\infty} h^2(x) \\frac{g(x)}{Z_g}dx - \\int_{0}^{\\infty} h(x)g(x)dx \\right]\n",
    "\\end{aligned}\n",
    "\n",
    "For $g(x) = \\frac{1}{2+(x-4)^2}$, the Monte Carlo variance is computed by\n",
    "$$\n",
    "\\text{Var}[{\\hat{I}}]  = \\frac{1}{N} \\left[ \\int_{0}^{\\infty} \\left[ x^4\\, \\sin\\left(\\sqrt{\\ln{(x+1)}}\\right)e^{-x}\\right]^2 \\frac{1}{2+(x-4)^2}dx - I \\right]\n",
    "$$\n",
    "For $g(x) = xe^{-x}$, the Monte Carlo variance is computed by\n",
    "$$\n",
    "\\text{Var}[{\\hat{I}}]  = \\frac{1}{N} \\left[ \\int_{0}^{\\infty} \\left[ \\frac{x^3\\, \\sin\\left(\\sqrt{\\ln{(x+1)}}\\right)}{2+(x-4)^2}\\right]^2 xe^{-x}dx - I \\right]\n",
    "$$\n",
    "\n",
    "Assuming a 10% burn-in rate and no thinning (i.e. $N = 15000$), using the general integration function, `scipy.integrate.quad`, we compute the Monte Carlo variances as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the value of I is:  6.03024985605\n",
      "the theoretical Monte Carlo variance (for 150,000 samples at 10% burn in) of first split is: 0.00113438899705\n",
      "the theoretical Monte Carlo variance (for 150,000 samples at 10% burn in) of second split is: 0.0104230274444\n"
     ]
    }
   ],
   "source": [
    "h_1 = lambda x: x**4 * np.sin(np.sqrt(np.log(x + 1))) * np.e**(-x) \n",
    "    \n",
    "h_2 = lambda x: x**3 * np.sin(np.sqrt(np.log(x + 1))) / (2 + (x - 4)**2)\n",
    "\n",
    "g_1 = lambda x: 1. / (2 + (x - 4)**2) if x >= 0 else 0\n",
    "\n",
    "g_2 = lambda x: x * np.e**(-x) if x >= 0 else 0\n",
    "\n",
    "h_times_g = lambda x: x**4 * np.sin(np.sqrt(np.log(x + 1))) * np.e**(-x)  / (2 + (x - 4)**2) \n",
    "\n",
    "I, _ = sp.integrate.quad(h_times_g, 0, np.inf)\n",
    "var_1, _ = sp.integrate.quad(lambda x: h_1(x)**2 * g_1(x), 0, np.inf)\n",
    "var_2, _ = sp.integrate.quad(lambda x: h_2(x)**2 * g_2(x), 0, np.inf)\n",
    "\n",
    "var_1 = (var_1 - I) / 15000.\n",
    "var_2 = (var_2 - I) / 15000.\n",
    "\n",
    "print 'the value of I is: ', I\n",
    "print 'the theoretical Monte Carlo variance (for 150,000 samples at 10% burn in) of first split is:', var_1\n",
    "print 'the theoretical Monte Carlo variance (for 150,000 samples at 10% burn in) of second split is:', var_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that we should expect the second split to yield a higher variance estimate. We can visualize the functions, $g$ and $h$, corresponding to each split and see if we can interpret the reason for this difference in variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Simulation\n",
    "\n",
    "For the first split, we choose $g(x) = \\frac{1}{2 + (x - 4)^2}$. As discussed in the above, we will actually work with a slightly re-defined version of $g$,\n",
    "$$\n",
    "\\tilde{g}(x) = \\begin{cases}\n",
    "\\frac{1}{2 + (x - 4)^2}, & x \\geq 0\\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "Note that $I = \\int_0^\\infty h(x)g(x)dx = \\int_{-\\infty}^\\infty h(x)\\tilde{g}(x)dx$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalization constant for first g_tilde:  1.98114048591\n"
     ]
    }
   ],
   "source": [
    "Z_1, _ = sp.integrate.quad(lambda x: 1. / (2 + (x - 4)**2), 0, np.inf)\n",
    "print 'normalization constant for first g_tilde: ', Z_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will choose normal distributions with variances of 10 for our proposal distribution. We will burn the first 10% of the samples and apply no thinning. \n",
    "\n",
    "Then applying Metropolis algorithm to sample from $\\tilde{g}$, and to compute $\\hat{I}$, is straight-forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metropolis(p, q, total_samples, x_init, burn_in):\n",
    "    samples = []\n",
    "    x_prev = x_init\n",
    "    accepted = 0\n",
    "    \n",
    "    for i in range(total_samples):\n",
    "        x_star = q(x_prev)\n",
    "        p_star = p(x_star)\n",
    "        p_prev = p(x_prev)\n",
    "        pdf_ratio = p_star / p_prev\n",
    "        \n",
    "        if np.random.uniform() < min(1, pdf_ratio):\n",
    "            samples.append(x_star)\n",
    "            x_prev = x_star\n",
    "            accepted += 1\n",
    "        else:\n",
    "            samples.append(x_prev)\n",
    "            \n",
    "    return np.array(samples[int(burn_in * total_samples):]), accepted * 1. / len(samples)\n",
    "\n",
    "#Monte Carlo \n",
    "def monte_carlo(h, Z, p, q, x_init, total_samples, burn_in):\n",
    "    samples, accept = metropolis(p, q, total_samples, x_init, burn_in)\n",
    "    print 'accept rate:', accept\n",
    "    return Z * np.mean(h(samples))\n",
    "\n",
    "#defining our proposal distribution\n",
    "sigma = 10\n",
    "q = lambda x: np.random.normal(x, sigma, 1)[0]\n",
    "\n",
    "#defining parameters of monte carlo simulation\n",
    "sims = 50\n",
    "x_init = 10\n",
    "total_samples = 150000\n",
    "burn_in = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation number: 0\n",
      "accept rate: 0.265626666667\n",
      "simulation number: 1\n",
      "accept rate: 0.27004\n",
      "simulation number: 2\n",
      "accept rate: 0.26934\n",
      "simulation number: 3\n",
      "accept rate: 0.263826666667\n",
      "simulation number: 4\n",
      "accept rate: 0.26562\n",
      "simulation number: 5\n",
      "accept rate: 0.264306666667\n",
      "simulation number: 6\n",
      "accept rate: 0.263546666667\n",
      "simulation number: 7\n",
      "accept rate: 0.263506666667\n",
      "simulation number: 8\n",
      "accept rate: 0.26458\n",
      "simulation number: 9\n",
      "accept rate: 0.262233333333\n",
      "simulation number: 10\n",
      "accept rate: 0.271713333333\n",
      "simulation number: 11\n",
      "accept rate: 0.2665\n",
      "simulation number: 12\n",
      "accept rate: 0.260793333333\n",
      "simulation number: 13\n",
      "accept rate: 0.263526666667\n",
      "simulation number: 14\n",
      "accept rate: 0.267666666667\n",
      "simulation number: 15\n",
      "accept rate: 0.260093333333\n",
      "simulation number: 16\n",
      "accept rate: 0.262566666667\n",
      "simulation number: 17\n",
      "accept rate: 0.271706666667\n",
      "simulation number: 18\n",
      "accept rate: 0.26614\n",
      "simulation number: 19\n",
      "accept rate: 0.26408\n",
      "simulation number: 20\n",
      "accept rate: 0.262473333333\n",
      "simulation number: 21\n",
      "accept rate: 0.264693333333\n",
      "simulation number: 22\n",
      "accept rate: 0.260453333333\n",
      "simulation number: 23\n",
      "accept rate: 0.2645\n",
      "simulation number: 24\n",
      "accept rate: 0.31148\n",
      "simulation number: 25\n",
      "accept rate: 0.266246666667\n",
      "simulation number: 26\n",
      "accept rate: 0.263626666667\n",
      "simulation number: 27\n",
      "accept rate: 0.263286666667\n",
      "simulation number: 28\n",
      "accept rate: 0.266586666667\n",
      "simulation number: 29\n",
      "accept rate: 0.26682\n",
      "simulation number: 30\n",
      "accept rate: 0.265086666667\n",
      "simulation number: 31\n",
      "accept rate: 0.26294\n",
      "simulation number: 32\n",
      "accept rate: 0.28752\n",
      "simulation number: 33\n",
      "accept rate: 0.272846666667\n",
      "simulation number: 34\n",
      "accept rate: 0.267633333333\n",
      "simulation number: 35\n",
      "accept rate: 0.26816\n",
      "simulation number: 36\n",
      "accept rate: 0.27956\n",
      "simulation number: 37\n",
      "accept rate: 0.263866666667\n",
      "simulation number: 38\n",
      "accept rate: 0.2642\n",
      "simulation number: 39\n",
      "accept rate: 0.26472\n",
      "simulation number: 40\n",
      "accept rate: 0.268746666667\n",
      "simulation number: 41\n",
      "accept rate: 0.26938\n",
      "simulation number: 42\n",
      "accept rate: 0.27036\n",
      "simulation number: 43\n",
      "accept rate: 0.262366666667\n",
      "simulation number: 44\n",
      "accept rate: 0.26456\n",
      "simulation number: 45\n",
      "accept rate: 0.288066666667\n",
      "simulation number: 46\n",
      "accept rate: 0.266413333333\n",
      "simulation number: 47\n",
      "accept rate: 0.265806666667\n",
      "simulation number: 48\n",
      "accept rate: 0.265526666667\n",
      "simulation number: 49\n",
      "accept rate: 0.2637\n",
      "*****\n",
      "monte carlo estimate of I:  6.03633820849\n",
      "variance of estimate:  0.00327298871874\n"
     ]
    }
   ],
   "source": [
    "#simulate 50 times, using first split\n",
    "means = []\n",
    "for i in range(sims):\n",
    "    print 'simulation number:', i\n",
    "    means.append(monte_carlo(h_1, Z_1, g_1, q, x_init, total_samples, burn_in))\n",
    "\n",
    "print '*****'\n",
    "print 'monte carlo estimate of I: ', np.mean(means)\n",
    "print 'variance of estimate: ', np.var(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation number: 0\n",
      "accept rate: 0.142053333333\n",
      "simulation number: 1\n",
      "accept rate: 0.14336\n",
      "simulation number: 2\n",
      "accept rate: 0.142146666667\n",
      "simulation number: 3\n",
      "accept rate: 0.141433333333\n",
      "simulation number: 4\n",
      "accept rate: 0.142146666667\n",
      "simulation number: 5\n",
      "accept rate: 0.143833333333\n",
      "simulation number: 6\n",
      "accept rate: 0.141493333333\n",
      "simulation number: 7\n",
      "accept rate: 0.143673333333\n",
      "simulation number: 8\n",
      "accept rate: 0.14288\n",
      "simulation number: 9\n",
      "accept rate: 0.143006666667\n",
      "simulation number: 10\n",
      "accept rate: 0.143273333333\n",
      "simulation number: 11\n",
      "accept rate: 0.141\n",
      "simulation number: 12\n",
      "accept rate: 0.14384\n",
      "simulation number: 13\n",
      "accept rate: 0.142806666667\n",
      "simulation number: 14\n",
      "accept rate: 0.143233333333\n",
      "simulation number: 15\n",
      "accept rate: 0.14422\n",
      "simulation number: 16\n",
      "accept rate: 0.143546666667\n",
      "simulation number: 17\n",
      "accept rate: 0.143653333333\n",
      "simulation number: 18\n",
      "accept rate: 0.143353333333\n",
      "simulation number: 19\n",
      "accept rate: 0.143246666667\n",
      "simulation number: 20\n",
      "accept rate: 0.14508\n",
      "simulation number: 21\n",
      "accept rate: 0.142793333333\n",
      "simulation number: 22\n",
      "accept rate: 0.144113333333\n",
      "simulation number: 23\n",
      "accept rate: 0.142506666667\n",
      "simulation number: 24\n",
      "accept rate: 0.143233333333\n",
      "simulation number: 25\n",
      "accept rate: 0.144353333333\n",
      "simulation number: 26\n",
      "accept rate: 0.142966666667\n",
      "simulation number: 27\n",
      "accept rate: 0.143933333333\n",
      "simulation number: 28\n",
      "accept rate: 0.14242\n",
      "simulation number: 29\n",
      "accept rate: 0.14364\n",
      "simulation number: 30\n",
      "accept rate: 0.143426666667\n",
      "simulation number: 31\n",
      "accept rate: 0.143206666667\n",
      "simulation number: 32\n",
      "accept rate: 0.141266666667\n",
      "simulation number: 33\n",
      "accept rate: 0.14006\n",
      "simulation number: 34\n",
      "accept rate: 0.14352\n",
      "simulation number: 35\n",
      "accept rate: 0.14108\n",
      "simulation number: 36\n",
      "accept rate: 0.140253333333\n",
      "simulation number: 37\n",
      "accept rate: 0.142946666667\n",
      "simulation number: 38\n",
      "accept rate: 0.143513333333\n",
      "simulation number: 39\n",
      "accept rate: 0.14392\n",
      "simulation number: 40\n",
      "accept rate: 0.143806666667\n",
      "simulation number: 41\n",
      "accept rate: 0.144146666667\n",
      "simulation number: 42\n",
      "accept rate: 0.14344\n",
      "simulation number: 43\n",
      "accept rate: 0.14344\n",
      "simulation number: 44\n",
      "accept rate: 0.142733333333\n",
      "simulation number: 45\n",
      "accept rate: 0.142313333333\n",
      "simulation number: 46\n",
      "accept rate: 0.144173333333\n",
      "simulation number: 47\n",
      "accept rate: 0.143473333333\n",
      "simulation number: 48\n",
      "accept rate: 0.14374\n",
      "simulation number: 49\n",
      "accept rate: 0.14214\n",
      "*****\n",
      "monte carlo estimate of I:  6.05033996164\n",
      "variance of estimate:  0.010227516708\n"
     ]
    }
   ],
   "source": [
    "#simulate 50 times, using second split\n",
    "means = []\n",
    "for i in range(sims):\n",
    "    print 'simulation number:', i\n",
    "    means.append(monte_carlo(h_2, 1., g_2, q, x_init, total_samples, burn_in))\n",
    "\n",
    "print '*****'\n",
    "print 'monte carlo estimate of I: ', np.mean(means)\n",
    "print 'variance of estimate: ', np.var(means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, the variance of the Monte Carlo estimator for the first split is lower than the variance for the second. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Metropolis Algorithm\n",
    "\n",
    "Suppose we ask you to memorize the order of the top five movies on IMDB. When we quiz you on the order afterwards, you might not recall the correct order, but the mistakes you will tend to make in your recall can be modeled by simple probabilistic models.\n",
    "  \n",
    "Let's say that the top five movies are:  \n",
    "1. *The Shawshank Redemption*\n",
    "2. *The Godfather*\n",
    "3. *The Godfather: Part II*\n",
    "4. *The Dark Knight*\n",
    "5. *Pulp Fiction*\n",
    "\n",
    "Let's represent this ordering by the vector $\\omega = (1,2,3,4,5)$. \n",
    "\n",
    "If you were to mistakenly recall the top five movies as:\n",
    "2. *The Godfather*\n",
    "3. *The Godfather: Part II*\n",
    "5. *Pulp Fiction*\n",
    "4. *The Dark Knight*\n",
    "1. *The Shawshank Redemption*\n",
    "\n",
    "We'd represent your answer by the vector $\\theta = (2,3,5,4,1)$.\n",
    "\n",
    "Now, we have a way of quantifying how wrong your answer can be. We define the Hamming distance between two top five rankings, $\\theta, \\omega$, as follows:\n",
    "$$d(\\theta, \\omega) = \\sum_{i=1}^5 \\mathbb{I}_{\\theta_i\\neq \\omega_i},$$ \n",
    "where $\\mathbb{I}_{\\theta_i\\neq \\omega_i}$ is the indicator function that returns 1 if $\\theta_i\\neq \\omega_i$, and 0 otherwise.\n",
    "\n",
    "For example, the Hamming distance between your answer and the correct answer is $d(\\theta, \\omega)=4$, because you only ranked *The Dark Knight* is correctly. \n",
    "\n",
    "Finally, let's suppose that the probability of giving a particular answer (expressed as $\\theta$) is modeled as\n",
    "$$ p(\\theta \\,|\\, \\omega, \\lambda) \\propto  e^{-\\lambda\\, d(\\theta,\\, \\omega)}.$$\n",
    "\n",
    "### Part A:\n",
    "\n",
    "Implement an Metropolis algorithm to produce sample guesses from 500 individuals, with various $\\lambda$ values, $\\lambda=0.2, 0.5, 1.0$. What are the top five possible guesses?\n",
    "\n",
    "### Part B:\n",
    "Compute the probability that *The Shawshank Redemption* is ranked as the top movie (ranked number 1) by the Metropolis algorithm sampler. Compare the resulting probabilities for the various different $\\lambda$ values. Summarize your findings.\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "Gonna use the same code for MH, with different targets and proposals and what not.\n",
    "\n",
    "Note that this proposal,\n",
    "$$ p(\\theta \\,|\\, \\omega, \\lambda) \\propto  e^{-\\lambda\\, d(\\theta,\\, \\omega)}.$$\n",
    "says that the closer (in edit distance) to the correct answer the more likely, more probably, that answer is. So this is a symmetric distribution that peaks at the correct answer.\n",
    "\n",
    "From the math, you can see right away that $\\lambda$ controls the \"peakiness\" like how concentrated the probability is around the correct answer (like how probable it is that you'd guess a super totally wrong one).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda =  0.2\n",
      "accept percent: 0.912\n",
      "top 5 guesses: ['[3 2 4 1 5]' '[4 1 3 2 5]' '[4 2 3 5 1]' '[2 3 4 5 1]' '[1 3 4 2 5]']\n",
      "guess frequencies: [16 11 10 10  9]\n",
      "\n",
      "\n",
      "\n",
      "lambda =  0.5\n",
      "accept percent: 0.702\n",
      "top 5 guesses: ['[1 2 3 4 5]' '[1 4 3 2 5]' '[1 5 3 4 2]' '[5 2 3 4 1]' '[1 2 3 5 4]']\n",
      "guess frequencies: [32 25 17 15 14]\n",
      "\n",
      "\n",
      "\n",
      "lambda =  1.0\n",
      "accept percent: 0.338\n",
      "top 5 guesses: ['[1 2 3 4 5]' '[3 2 1 4 5]' '[2 1 3 4 5]' '[1 2 4 3 5]' '[1 2 3 5 4]']\n",
      "guess frequencies: [225  31  12  11  10]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#the correct answer\n",
    "omega = np.array([1, 2, 3, 4, 5])\n",
    "#proposal distribution\n",
    "proposal = lambda theta: np.random.permutation(theta)\n",
    "#Hamming distance from correct answer\n",
    "d = lambda theta: len((theta - omega)[(theta - omega) != 0])\n",
    "#target distribution in terms of theta and lambda\n",
    "target_joint = lambda theta, l: np.e**(-l * d(theta))\n",
    "#total number of guesses to simulate\n",
    "total_samples = 500\n",
    "#initial guess\n",
    "x_init = np.array([2, 1, 3, 5, 4])\n",
    "\n",
    "#lambdas to try\n",
    "lambdas = [0.2, 0.5, 1.0]\n",
    "\n",
    "#simulate 500 guesses for each lambda value\n",
    "for l in lambdas:\n",
    "    target = partial(target_joint, l=l)    \n",
    "    print 'lambda = ', l\n",
    "    \n",
    "    #sample\n",
    "    samples, accept = metropolis(target, proposal, total_samples, x_init, 0)\n",
    "    print 'accept percent:', accept\n",
    "    \n",
    "    #get top 5 guesses in sample\n",
    "    samples = [np.array_str(s) for s in samples]    \n",
    "    vals, counts = np.unique(samples, return_counts=True)\n",
    "    top_5 = counts.argsort()[-5:][::-1]\n",
    "    \n",
    "    print 'top 5 guesses:', vals[top_5]\n",
    "    print 'guess frequencies:', counts[top_5]\n",
    "    print '\\n\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the larger the lambda, the more likely you're to guess the right answer and the more tightly you're guesses will cluster around the right answer.\n",
    "\n",
    "To compute $p(\\text{shawshank is top} | lambda)$, we count how many guesses have 1 in the first entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda =  0.2\n",
      "p(shawshank ranked top | lambda)= 0.294\n",
      "\n",
      "\n",
      "\n",
      "lambda =  0.5\n",
      "p(shawshank ranked top | lambda)= 0.336\n",
      "\n",
      "\n",
      "\n",
      "lambda =  1.0\n",
      "p(shawshank ranked top | lambda)= 0.494\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for l in lambdas:\n",
    "    target = partial(target_joint, l=l)\n",
    "    print 'lambda = ', l\n",
    "    \n",
    "    #sample\n",
    "    samples, accept = metropolis(target, proposal, total_samples, x_init, 0)\n",
    "    #get top ranked movie for samples\n",
    "    top_guesses = samples[:, 0]\n",
    "    \n",
    "    print 'p(shawshank ranked top | lambda)=', len(top_guesses[top_guesses == 1])  * 1. / len(samples)\n",
    "    print '\\n\\n'\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, $p(\\text{shawshank is top} | lambda)$ increases as lambda increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
