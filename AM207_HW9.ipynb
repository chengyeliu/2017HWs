{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AM 207** Homework 9\n",
    "\n",
    "This homework is designed so that you can work through the proababilistic aspects of machine learning, and to understand the choices being made when you carry out a classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1. Separate the bayesian irises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this problem is to introduce you to the idea of classification \n",
    "using Bayesian inferences. \n",
    "\n",
    "You are given the famous *Fisher flower Iris data set*\n",
    "which is a  multivariate data set introduced by Sir Ronald Fisher (1936) as an example of discriminant analysis.\n",
    "The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. Based on the combination of these four features, you will build a model to predict\n",
    "the species. \n",
    "\n",
    "For this problem only consider two classes: virginica and not-virginica. \n",
    "\n",
    "The iris data can be obtained [here](iris.txt).\n",
    "\n",
    "Lets $(X, Y )$ be our dataset, where $X=\\{\\vec{x}_1, \\ldots \\vec{x}_n\\}$ and $\\vec{x}_i$ is a 5D vector corresponding to an offset 1 and\n",
    "the four components explained above. $Y \\in \\{0,1\\}$ are the scalar \n",
    "labels of a class. In other words  \n",
    "the species labels are your $Y$ data (say virginica = 0 and virginica=1), and the four features, petal length\n",
    "and width, sepal length and width, along with the offset, are your $X$ data. \n",
    "\n",
    "The goal is to train a classifier, that will predict an unknown class label $\\hat{y}$ from a new data point $x$. \n",
    "\n",
    "Consider the following model (logistic model) for the probability of a class:\n",
    "\n",
    "$$ Y \\sim \\frac{1}{1+e^{-X^T \\beta}} $$\n",
    "\n",
    "where $\\beta$ is a 5D parameter to learn. \n",
    "\n",
    "Use a 60-40 stratified (preserving class membership) split of the dataset into a training set and a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Sampling\n",
    "\n",
    "1. Choose a prior for $\\beta \\sim N(0, \\sigma^2 I) $ and write down the formula for the normalized posterior $p(\\beta| Y,X)$. \n",
    "2. Find the MAP estimate for the posterior on the training set.\n",
    "3. Implement a  sampler to sample from this posterior of $\\beta$.   Generate samples of $\\beta$ and plot the sequence of $\\beta$'s  and histograms for each $\\beta$ component.\n",
    "4. Propagate both the MAP estimate and posterior on $\\beta$s to a set of probabilities and posterior distributions for each data point $x$ on the training set. Plot the posterior against the MAP estimate for 2-3 of your data points, preferably those with MAP estimates closer to 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now that we have both a point estimate and posterior distribution on the probability of a sample being virginica, we can use these to make predictions on both the training set and test set.\n",
    "\n",
    "**There are two ways to make these predictions, given an estimate of $p(y=1 \\vert x)$ **: \n",
    "\n",
    "(A) You can sample from the Bernoulli likelihood at the data point $x$ to decide if that particular data points classification $y(x)$ should be a 1 or a 0.\n",
    "\n",
    "(B) Or you could do the intuitive \"machine-learning-decision-theoretic\" (MLDT) thing, where you assign a data  point $x$ a classification 1 if $p(y=1 \\vert x) > 0.5$.\n",
    "\n",
    "**And there are multiple ways in which you can do these probability estimates at a sample $x$**:\n",
    "\n",
    "(A) You could just use the MAP value, ($p_{MAP}$), or the posterior mean ($p_{MEAN}$)\n",
    "\n",
    "(B) You can see what fraction of your posterior samples have values above 0.5 (ie you are calculating 1-cdf(0.5) on the posterior ($p_{CDF}$)\n",
    "\n",
    "(C) Both these above methods miss the combined smearing of the posterior and sampling distributions. In other words they dont sample from the posterior predictive. If we draw a large number of samples from the posterior predictive distribution at a data point $x$, the fraction of 1s will give an estimate for the probability to use, $p_{PP}$, which is different from the MAP estimate, or the CDF estimate.\n",
    "\n",
    "## Part B: Playing about with probabilities\n",
    "\n",
    "1. Plot the distribution of $p_{MEAN}$,$p_{CDF}$, and $p_{PP}$ over all the data points in the training set. How are these different?\n",
    "2. Plot the posterior-predictive distribution of the misclassification rate with respect to the true class identities $y(x)$ of the data points $x$ (in other words you are plotting a histogram with the misclassification rate for the $n_{trace}$ posterior-predictive samples) on the training set.\n",
    "3. Make the same plot from the posterior, rather than the posterior predictive, by using the MLDT defined above. Overlay this plot on the previous one. That is, for every posterior sample, consider whether the data point ought to be classified as a 1 or 0 from the $p>0.5 \\implies y=1$ decision theoretic prespective. Compare with your previous diagram. Which case (from posterior-predictive or from-posterior) has a wider mis-classification distribution? \n",
    "4. Is the classification at the true training data points (you can think of this as a bitstring '100101...' 90 characters long) represented in the posterior predictive trace? If so, how many times? Is it the most frequent string in the traces? Explain your conclusions.\n",
    "5. Repeat 2 and 3 for the test set, ie make predictions `:-)` . What do you find about the width of the distributions?\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Deriving the MLDT\n",
    "\n",
    "If you took a distribution of prediction to your boss, he/she would probably ask you to make a choice. Decision Theory is the right way to make such a choice, rather than choosing an arbitrary set of bernoulli samplings. The idea is to minimize the posterior predictive averaged decision risk at each sample $x$:\n",
    "\n",
    "$$R(g, x) = \\int dy P(y \\vert x) R(g, y \\vert x) = \\sum_{y=0,1} P(y \\vert x) R(g, y \\vert x)$$\n",
    "\n",
    "where $g$ is the action you take: classify as 1 or 0. Use $R(g, y \\vert x) = {\\mathbb 1}_{g \\ne y} \\forall x$, the 1-0 loss.\n",
    "\n",
    "1. Show that if $R(1, x) <= R(0, x)$ and thus we choose 1 as our classification, $p(1 \\vert x) >= 0.5$. \n",
    "2. Use this MLDT rule and probabilities from both $p_{CDF}$ and $p_{PP}$ to make two classifications on the training set and test set. What is the misclassification rate in both? WHy do you think this is the case? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
