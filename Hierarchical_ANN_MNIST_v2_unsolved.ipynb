{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AMPTH 207: Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "\n",
    "### Final exam\n",
    "\n",
    "**Harvard University**  \n",
    "**Spring 2017**  \n",
    "**Instructor: Rahul Dave**  \n",
    "**Due Date:** \n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers as well as your iPython notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1. A Hierarchical Artificial Neural Network Using Variational Inference\n",
    "\n",
    "We have learned in this course to create automatic classifiers. In such supervised algorithms,  a function is learned that maps certain input data to a set of outputs, usually the classes into which the dataset wants to be separated. This is achieved, in the case of a Artificial Neural Network like the Multi-Layer Perceptron by iteratively updating the weights that regulate the flow of information between the different layers of the artificial neural network (ANN). We have also seen that it is possible to regularize by penalizing some weights in order to avoid overfitting. This regularization is equivalent to imposing priors on the weights when they are initialized.\n",
    "\n",
    "This approach is inherently Bayesian. We can specify priors to inform and constrain our models and get uncertainty estimations in form of a posterior distribution. Using MCMC sampling algorithms we can draw samples from this posterior to very flexibly estimate these models. This also means that we can obtain uncertainties on both the weights and the classes that we obtain from the output layer.\n",
    "\n",
    "In this problem you are asked to build a hierarchical Bayesian ANN to classify the MNIST dataset (properly split into training and test sets) that we have used before, and to infer uncertainties for the weighths and the resulting classes. Variational inference can become very handy here: instead of drawing samples from the posterior, these algorithms fit a distribution (e.g. normal) to the posterior turning a sampling problem into and optimization problem. ADVI -- Automatic Differentation Variational Inference is implemented in PyMC3 and Stan. \n",
    "\n",
    "Read this [blog post](http://twiecki.github.io/blog/2016/07/05/bayesian-deep-learning/) to get an idea about the problem, and to see how some of the ADVI related technology is set up.\n",
    "\n",
    "### Part A\n",
    "\n",
    "In Long HW 1 you built a multi-layer perceptron to classify the MNIST dataset.\n",
    "\n",
    "Here we will build a Bayesian Artificial Neural Network. \n",
    "\n",
    "Build an artificial neural network in pymc3 (or Stan, your choice) with two hidden layers and 25 neurons in each, and use it to classify the MNIST dataset. Use the $\\tanh$ function as the non-linearity, and initialize ALL the weights using using normal priors with $\\mu=0$ and two different values for $\\sigma = 0.03, 0.1$.\n",
    "\n",
    "1. You could use a MCMC sampler such as NUTS to train your model. But this could be extremely slow as you add more layer or more units. Instead, use the PyMC3/Stan ADVI implementation to approximate the posteriors of the weights, and obtain the posterior means, standard deviations, and the evidence lower bound (ELBO), which is your objective function. Use mini-batch gradient descent for the optimization (a minibatch size of 500 is appropriate), and a todal of 50000 ADVI iterations.\n",
    "\n",
    "2. Plot the objective function (ELBO) as a function of the iteration. Using the best fit from your training, predict on the test set by estimating posterior predictives for the classes in each case, and provide the accuracy of your classification. How does your accuracy compare for the two values of $\\sigma$? *Hint: if you need traces to perform inference, you can sample directly from the normal distribution that ADVI outputs*.\n",
    "\n",
    "\n",
    "### Part B\n",
    "The connection between the standard deviation of the weight prior to the strengh of the L2 penalization term leads to an interesting idea. In Part A we fixed $\\sigma=0.03, 0.1$, but this is a somehow arbitrary decision. Perhaps $\\sigma$ should be different for each layer, and have a different magnitude. Rather than randomly trying different $\\sigma$s, we can learn the best values directly from the data. This is when the ANN becomes hierarchical. \n",
    "\n",
    "1. Modify your algorithm to include different hyperpriors for the $\\sigma$s for the bias and weights in each layer, and use ADVI again to optimize the model. Plot the posteriors distributions for the hyperpriors.\n",
    "\n",
    "2. Just as in part A, predict on the test data and report your accuracy.\n",
    "\n",
<<<<<<< HEAD
    "3. You can now use the fact that we're in a Bayesian framework and explore uncertainty in your predictions. A $\\chi^2$ statistic is used to investigate whether distributions of categorical variables differ from one another (see http://math.hws.edu/javamath/ryan/ChiSquare.html). Use the $\\chi^2$ statistic to compare the distributions of predicted versus real classes for those predictions that you got right with respect to the test set, and for those that you got wrong. Plot a histogram of $\\chi^2$ statistics for both cases. What do you conclude in terms of the uncertainties of the predictions? Is the missclassification rate uniform across classes?"
=======
    "3. You can now use the fact that we're in a Bayesian framework and explore uncertainty in your predictions. Compute the $\\chi^2$ statistic for the predictions and see how uniform the samples are. The more uniform, the higher our uncertainty. Plot histograms of the $\\chi^2$ statistic for those predictions that you got right with respect to the test set, and for those that you got wrong. What is your conclusion?\n",
    "\n",
    "\n",
    "\n"
>>>>>>> 7eb6e056a4b84060b3cb2fb571ed02c9b1018fbf
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pymc3 as pm\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "import gzip\n",
    "import pickle\n",
    "from scipy.stats import mode, chisquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "dataset='mnist.pkl.gz'\n",
    "with gzip.open(dataset, 'rb') as f:\n",
    "        try:\n",
    "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "        except:\n",
    "            train_set, valid_set, test_set = pickle.load(f)\n",
    "            \n",
    "X_train = train_set[0]\n",
    "Y_train = train_set[1]\n",
    "\n",
    "X_test = test_set[0]\n",
    "Y_test = test_set[1]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
