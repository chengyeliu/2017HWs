{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APMTH 207: Advanced Scientific Computing: \n",
    "## Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "## Homework #5\n",
    "**Harvard University**<br>\n",
    "**Spring 2017**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "**Due Date: ** Thursday, March 2nd, 2017 at 11:59pm\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers as well as your iPython notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import stats \n",
    "from scipy.stats import multivariate_normal\n",
    "import time\n",
    "import math\n",
    "from functools import partial\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problem 1: Monte Carlo and Simulation Revisited\n",
    "In Homework #2, we used simulation to compute the expected values of functions of random variables. That is, given a random variable $X$, defined over $\\mathbb{R}$, distributed according to the pdf $f_X$, and given a real-valued function of $X$, $h(X)$, we approximated $\\mathbb{E}[h(X)]$ as follows\n",
    "$$\n",
    "\\mathbb{E}[h(X)] = \\int_{\\mathbb{R}} h(x)f_X(x) dx \\approx \\frac{1}{N} \\sum_{i=1}^N h(X_i), \\quad X_i \\sim f_X\n",
    "$$\n",
    "\n",
    "Now, suppose that, instead of being given the distribution $f_X$ and $h(X)$, you were simply asked to evaluate the following complex integral:\n",
    "$$\n",
    "I=\\int_{0}^{\\infty} \\frac{x^4\\, \\sin\\left(\\sqrt{\\ln{(x+1)}}\\right)e^{-x}}{2+(x-4)^2} \\, dx \n",
    "$$\n",
    "A clever way to apply our Monte Carlo techniques would be to split the integrand as $h(x)f_X(x)$, and then approximate the integral as we have done in Homework #2:\n",
    "$$\n",
    "I = \\int_{0}^{\\infty} h(x)\\,f_X(x) dx  \\approx \\frac{1}{N} \\sum\\limits_{i=1}^{N} h(X_i)$$ \n",
    "where the $X_i$'s are independently drawn from $f_X(x)$. \n",
    "\n",
    "We denote the approximation of the integral as follows\n",
    "$$\\hat{I} = \\frac{1}{N} \\sum\\limits_{i=1}^{N} h(X_i), \\quad X_i \\sim f_X.$$\n",
    "\n",
    "\n",
    "### Part A:\n",
    "\n",
    "Rewrite your integrand as a product of two functions, $h(x)g(x)$, which can then be expressed as $h(x)f_X(x)$, where $f_X$ is a pdf (you may use one of the splits we propose in Part B or create your own). Explain why your choice of $h$ is appropriate. Explain why your choice of $g$ is appropriate for creating a pdf $f_X$.\n",
    "\n",
    "(**Hint:** think about what you would have to do do turn $g$ into a good pdf and $h$ into a function that can be evaluated at multiple samples from this pdf. Think about how to choose these two functions to make your Monte Carlo approximation of $I$ as accurate as possible.)\n",
    "\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "After splitting, your integral will look like\n",
    "$$\n",
    "I = \\int_{0}^{\\infty} h(x) g(x) dx\n",
    "$$\n",
    "but you can't do Monte Carlo yet b/c $g$ is not a pdf! So let's turn it into one. First compute the normalizing constant for $g$ (i.e. aread under $g$)\n",
    "$$\n",
    "N_g = \\int_{\\mathbb{R}} g(x) dx\n",
    "$$\n",
    "Then define a pdf based on $g$\n",
    "$$\n",
    "f_X = \\frac{g(x)}{N_g}\n",
    "$$\n",
    "So now the integral becomes:\n",
    "$$\n",
    "I = N_g \\int_{0}^{\\infty} h(x) \\frac{g(x)}{N_g} dx = N_g \\int_{0}^{\\infty} h(x) f_X(x) dx = \\mathbb{E}_{f_X}[h(X)] \\approx \\frac{1}{n}\\sum_{i=1}^n h(X_i), \\quad X_i \\sim f_X\n",
    "$$\n",
    "**Note:** Don't forget to multiply your tranformed integral by the normalizing constant $N_g$!!!\n",
    "\n",
    "So generally, you want:\n",
    "1. $g$ to be easy to integrate, since you need to integrate it to compute $N_g$\n",
    "2. you want $g$ to be easy to sample from if possible, since you need to sample from it\n",
    "3. you want $h$ to be flat (small variance) over your $g$ since $Var[\\hat{I}] = \\frac{Var[h(X)]}{n}$\n",
    "4. there are a whole bunch of other factors to consider, like where should the probability mass of $g$ be? where $h$ has high variance or low? But we are not requiring this kind of depth.\n",
    "\n",
    "### Part B:\n",
    "\n",
    "- Use $\\frac{1}{2+(x-4)^2}$ to create your pdf $f_X$. Implement a Metropolis algorithm to sample from $f_X$. Run the simulation 50 times for 150,000 points. Report the value of $\\hat{I}$ and that of Var[${\\hat{I}}$].\n",
    "\n",
    "\n",
    "- Use $xe^{-x}$ to create your pdf $f_X$. Implement a Metropolis algorithm to sample from $f_X$. Run the simulation 50 times for 150,000 points. Report the value of $\\hat{I}$ and that of Var[${\\hat{I}}$].\n",
    "\n",
    "\n",
    "- Compare the variance of your two estimates. Which choice of $f_X$ is better? Explain why.\n",
    "\n",
    "**Solution:** \n",
    "\n",
    "**BE CAREFUL HERE!!!** You might be tempted to  willy nilly sample $g$ using a normal proposal distribution. But a normal distribution will propose negative values, and the integral is from 0 to infinity, so what do you do with your negative samples??? ***What not to do*** sample a bunch of stuff and then at the end throw away negative values before averaging the values of $h$. This is bad b/c now your truncated chain (the array of samples) is no longer generated by a irreducible markov system (that satisfies detailed balance)!!! The rigorous way to handle this is to re-define your g(x) and h(x) so that they are only supported on non-negative numbers!\n",
    "$$\n",
    "\\tilde{g}(x) = \\begin{cases}\n",
    "g(x), & x>=0\\\\\n",
    "0, & x < 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\tilde{h}(x) = \\begin{cases}\n",
    "h(x), & x>=0\\\\\n",
    "0, & x < 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "***What not to do*** change the MH algorithm so that it automatically rejects negative proposals. In our special example, hard-coding a rejection in your MH algo is equivalent to defining $g$ to be zero on negative numbers, but hard-coding should be avoided as it can **esaily** correspond to changing the proposal distribution in a way that violates detailed-balance or irreducibility. The point here is that when tweaking M-H, try to tweak the math (the function definitions), NOT the algo (unless you are a sampling ninja and are willing to write proofs).\n",
    "\n",
    "Again, take Rahul's code with a couple of lines of modification (we're gonna do 10% burn-in and no thinning, starting at x=10, with normal proposal with sigma 10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theoretical variance of my monte carlo estimator with first h 0.00149068527173\n",
      "theoretical variance of my monte carlo estimator with second h 0.00840078321307\n"
     ]
    }
   ],
   "source": [
    "p = lambda x: 1. / (2 + (x - 4)**2)\n",
    "sigma = 10\n",
    "q = lambda x: np.random.normal(x, sigma, 1)[0]\n",
    "sims = 50\n",
    "N = 1.9811\n",
    "\n",
    "def h_1(x):\n",
    "    if x >= 0:\n",
    "        return x**4 * np.sin(np.sqrt(np.log(x + 1))) * np.e**(-x) \n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def h_2(x):\n",
    "    if x >= 0:\n",
    "        return x**3 * np.sin(np.sqrt(np.log(x + 1))) / (2 + (x - 4)**2)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def f(x):\n",
    "    if x >= 0:\n",
    "        return x**4 * np.sin(np.sqrt(np.log(x + 1))) * np.e**(-x) / (2 + (x - 4)**2)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "exp_val, _ = sp.integrate.quad(f, 0, np.inf)\n",
    "\n",
    "def integrand_h_1(x):\n",
    "    if x >= 0:\n",
    "        return (x**4 * np.sin(np.sqrt(np.log(x + 1))) * np.e**(-x) - exp_val)**2/ (2 + (x - 4)**2)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def integrand_h_2(x):\n",
    "    if x >= 0:\n",
    "        return (x**3 * np.sin(np.sqrt(np.log(x + 1))) / (2 + (x - 4)**2)  - exp_val)**2 * x * np.e**(-x)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "\n",
    "var, _ = sp.integrate.quad(integrand_h_1, 0, np.inf)\n",
    "\n",
    "print 'theoretical variance of my monte carlo estimator with first h', var/15000\n",
    "\n",
    "\n",
    "var, _ = sp.integrate.quad(integrand_h_2, 0, np.inf)\n",
    "\n",
    "print 'theoretical variance of my monte carlo estimator with second h', var/15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is saying that we expect the second $h$ gives us higher variance, which may be surprising, but we'll accept any reasonable interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalization constant for first h 1.98114048591\n"
     ]
    }
   ],
   "source": [
    "N, _ = sp.integrate.quad(lambda x: 1. / (2 + (x - 4)**2), 0, np.inf)\n",
    "print 'normalization constant for first h', N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation number: 0\n",
      "accept rate: 0.277453333333\n",
      "simulation number: 1\n",
      "accept rate: 0.258313333333\n",
      "simulation number: 2\n",
      "accept rate: 0.264933333333\n",
      "simulation number: 3\n",
      "accept rate: 0.270706666667\n",
      "simulation number: 4\n",
      "accept rate: 0.26402\n",
      "simulation number: 5\n",
      "accept rate: 0.268626666667\n",
      "simulation number: 6\n",
      "accept rate: 0.263986666667\n",
      "simulation number: 7\n",
      "accept rate: 0.2676\n",
      "simulation number: 8\n",
      "accept rate: 0.26822\n",
      "simulation number: 9\n",
      "accept rate: 0.2676\n",
      "simulation number: 10\n",
      "accept rate: 0.268913333333\n",
      "simulation number: 11\n",
      "accept rate: 0.267226666667\n",
      "simulation number: 12\n",
      "accept rate: 0.266846666667\n",
      "simulation number: 13\n",
      "accept rate: 0.27466\n",
      "simulation number: 14\n",
      "accept rate: 0.272446666667\n",
      "simulation number: 15\n",
      "accept rate: 0.264506666667\n",
      "simulation number: 16\n",
      "accept rate: 0.266273333333\n",
      "simulation number: 17\n",
      "accept rate: 0.267006666667\n",
      "simulation number: 18\n",
      "accept rate: 0.26434\n",
      "simulation number: 19\n",
      "accept rate: 0.264093333333\n",
      "simulation number: 20\n",
      "accept rate: 0.271926666667\n",
      "simulation number: 21\n",
      "accept rate: 0.27374\n",
      "simulation number: 22\n",
      "accept rate: 0.2666\n",
      "simulation number: 23\n",
      "accept rate: 0.265573333333\n",
      "simulation number: 24\n",
      "accept rate: 0.267453333333\n",
      "simulation number: 25\n",
      "accept rate: 0.2632\n",
      "simulation number: 26\n",
      "accept rate: 0.2774\n",
      "simulation number: 27\n",
      "accept rate: 0.262846666667\n",
      "simulation number: 28\n",
      "accept rate: 0.26436\n",
      "simulation number: 29\n",
      "accept rate: 0.264373333333\n",
      "simulation number: 30\n",
      "accept rate: 0.26548\n",
      "simulation number: 31\n",
      "accept rate: 0.267286666667\n",
      "simulation number: 32\n",
      "accept rate: 0.26472\n",
      "simulation number: 33\n",
      "accept rate: 0.26264\n",
      "simulation number: 34\n",
      "accept rate: 0.263546666667\n",
      "simulation number: 35\n",
      "accept rate: 0.263513333333\n",
      "simulation number: 36\n",
      "accept rate: 0.260806666667\n",
      "simulation number: 37\n",
      "accept rate: 0.266086666667\n",
      "simulation number: 38\n",
      "accept rate: 0.267026666667\n",
      "simulation number: 39\n",
      "accept rate: 0.263846666667\n",
      "simulation number: 40\n",
      "accept rate: 0.26774\n",
      "simulation number: 41\n",
      "accept rate: 0.265026666667\n",
      "simulation number: 42\n",
      "accept rate: 0.268126666667\n",
      "simulation number: 43\n",
      "accept rate: 0.26878\n",
      "simulation number: 44\n",
      "accept rate: 0.263546666667\n",
      "simulation number: 45\n",
      "accept rate: 0.2684\n",
      "simulation number: 46\n",
      "accept rate: 0.275973333333\n",
      "simulation number: 47\n",
      "accept rate: 0.262933333333\n",
      "simulation number: 48\n",
      "accept rate: 0.259846666667\n",
      "simulation number: 49\n",
      "accept rate: 0.275466666667\n",
      "6.03652740871\n",
      "0.00186481246249\n"
     ]
    }
   ],
   "source": [
    "#Rahul's code for Metropolis:\n",
    "\n",
    "def metropolis(p, qdraw, nsamp, xinit):\n",
    "    samples=np.empty(nsamp)\n",
    "    x_prev = xinit\n",
    "    accepted=0\n",
    "    for i in range(nsamp):\n",
    "        x_star = qdraw(x_prev)\n",
    "        p_star = p(x_star)\n",
    "        p_prev = p(x_prev)\n",
    "        pdfratio = p_star/p_prev\n",
    "        if np.random.uniform() < min(1, pdfratio):\n",
    "            samples[i] = x_star\n",
    "            x_prev = x_star\n",
    "            accepted += 1\n",
    "        else:#we always get a sample\n",
    "            samples[i]= x_prev\n",
    "            \n",
    "    return samples[15000:], accepted * 1. / len(samples)\n",
    "\n",
    "def monte_carlo(h, N, p, q):\n",
    "    x_init = 10\n",
    "    num_samples = 150000\n",
    "    samples, accept = metropolis(p, q, num_samples, x_init)\n",
    "    print 'accept rate:', accept\n",
    "    return N * np.mean(map(h, list(samples)))\n",
    "        \n",
    "    \n",
    "# THE FIRST SPLIT!!!!!\n",
    "def p(x):\n",
    "    if x >= 0:\n",
    "        return 1. / (2 + (x - 4)**2)\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "sigma = 10\n",
    "q = lambda x: np.random.normal(x, sigma, 1)[0]\n",
    "sims = 50\n",
    "\n",
    "def h(x):\n",
    "    if x >= 0:\n",
    "        return x**4 * np.sin(np.sqrt(np.log(x + 1))) * np.e**(-x) \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "means = []\n",
    "for i in range(sims):\n",
    "    print 'simulation number:', i\n",
    "    means.append(monte_carlo(h, N, p, q))\n",
    "\n",
    "    \n",
    "print np.mean(means)\n",
    "print np.var(means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation number: 0\n",
      "accept rate: 0.141493333333\n",
      "simulation number: 1\n",
      "accept rate: 0.14108\n",
      "simulation number: 2\n",
      "accept rate: 0.142986666667\n",
      "simulation number: 3\n",
      "accept rate: 0.1417\n",
      "simulation number: 4\n",
      "accept rate: 0.140526666667\n",
      "simulation number: 5\n",
      "accept rate: 0.143706666667\n",
      "simulation number: 6\n",
      "accept rate: 0.142473333333\n",
      "simulation number: 7\n",
      "accept rate: 0.142113333333\n",
      "simulation number: 8\n",
      "accept rate: 0.145633333333\n",
      "simulation number: 9\n",
      "accept rate: 0.143793333333\n",
      "simulation number: 10\n",
      "accept rate: 0.143486666667\n",
      "simulation number: 11\n",
      "accept rate: 0.143993333333\n",
      "simulation number: 12\n",
      "accept rate: 0.143133333333\n",
      "simulation number: 13\n",
      "accept rate: 0.141926666667\n",
      "simulation number: 14\n",
      "accept rate: 0.144313333333\n",
      "simulation number: 15\n",
      "accept rate: 0.14368\n",
      "simulation number: 16\n",
      "accept rate: 0.1424\n",
      "simulation number: 17\n",
      "accept rate: 0.143533333333\n",
      "simulation number: 18\n",
      "accept rate: 0.141213333333\n",
      "simulation number: 19\n",
      "accept rate: 0.141573333333\n",
      "simulation number: 20\n",
      "accept rate: 0.14156\n",
      "simulation number: 21\n",
      "accept rate: 0.144106666667\n",
      "simulation number: 22\n",
      "accept rate: 0.14438\n",
      "simulation number: 23\n",
      "accept rate: 0.142833333333\n",
      "simulation number: 24\n",
      "accept rate: 0.142666666667\n",
      "simulation number: 25\n",
      "accept rate: 0.142373333333\n",
      "simulation number: 26\n",
      "accept rate: 0.144166666667\n",
      "simulation number: 27\n",
      "accept rate: 0.143353333333\n",
      "simulation number: 28\n",
      "accept rate: 0.142613333333\n",
      "simulation number: 29\n",
      "accept rate: 0.14304\n",
      "simulation number: 30\n",
      "accept rate: 0.143166666667\n",
      "simulation number: 31\n",
      "accept rate: 0.14384\n",
      "simulation number: 32\n",
      "accept rate: 0.14312\n",
      "simulation number: 33\n",
      "accept rate: 0.141653333333\n",
      "simulation number: 34\n",
      "accept rate: 0.142026666667\n",
      "simulation number: 35\n",
      "accept rate: 0.14328\n",
      "simulation number: 36\n",
      "accept rate: 0.14334\n",
      "simulation number: 37\n",
      "accept rate: 0.14016\n",
      "simulation number: 38\n",
      "accept rate: 0.142486666667\n",
      "simulation number: 39\n",
      "accept rate: 0.14324\n",
      "simulation number: 40\n",
      "accept rate: 0.1429\n",
      "simulation number: 41\n",
      "accept rate: 0.142506666667\n",
      "simulation number: 42\n",
      "accept rate: 0.14114\n",
      "simulation number: 43\n",
      "accept rate: 0.142866666667\n",
      "simulation number: 44\n",
      "accept rate: 0.14306\n",
      "simulation number: 45\n",
      "accept rate: 0.14166\n",
      "simulation number: 46\n",
      "accept rate: 0.14258\n",
      "simulation number: 47\n",
      "accept rate: 0.14354\n",
      "simulation number: 48\n",
      "accept rate: 0.143706666667\n",
      "simulation number: 49\n",
      "accept rate: 0.1434\n",
      "6.02026275276\n",
      "0.00997762895952\n"
     ]
    }
   ],
   "source": [
    "# THE SECOND SPLIT!!!!!\n",
    "def p(x):\n",
    "    if x >= 0:\n",
    "        return x * np.e**(-x)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "sigma = 10\n",
    "q = lambda x: np.random.normal(x, sigma, 1)[0]\n",
    "sims = 50\n",
    "N = 1\n",
    "\n",
    "def h(x):\n",
    "    if x >= 0:\n",
    "        return x**3 * np.sin(np.sqrt(np.log(x + 1))) / (2 + (x - 4)**2)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "means = []\n",
    "for i in range(sims):\n",
    "    print 'simulation number:', i\n",
    "    means.append(monte_carlo(h, N, p, q))\n",
    "\n",
    "    \n",
    "print np.mean(means)\n",
    "print np.var(means)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the variances turned out like we (theoretically expected). Yay!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Metropolis Algorithm\n",
    "\n",
    "Suppose we ask you to memorize the order of the top five movies on IMDB. When we quiz you on the order afterwards, you might not recall the correct order, but the mistakes you will tend to make in your recall can be modeled by simple probabilistic models.\n",
    "  \n",
    "Let's say that the top five movies are:  \n",
    "1. *The Shawshank Redemption*\n",
    "2. *The Godfather*\n",
    "3. *The Godfather: Part II*\n",
    "4. *The Dark Knight*\n",
    "5. *Pulp Fiction*\n",
    "\n",
    "Let's represent this ordering by the vector $\\omega = (1,2,3,4,5)$. \n",
    "\n",
    "If you were to mistakenly recall the top five movies as:\n",
    "2. *The Godfather*\n",
    "3. *The Godfather: Part II*\n",
    "5. *Pulp Fiction*\n",
    "4. *The Dark Knight*\n",
    "1. *The Shawshank Redemption*\n",
    "\n",
    "We'd represent your answer by the vector $\\theta = (2,3,5,4,1)$.\n",
    "\n",
    "Now, we have a way of quantifying how wrong your answer can be. We define the Hamming distance between two top five rankings, $\\theta, \\omega$, as follows:\n",
    "$$d(\\theta, \\omega) = \\sum_{i=1}^5 \\mathbb{I}_{\\theta_i\\neq \\omega_i},$$ \n",
    "where $\\mathbb{I}_{\\theta_i\\neq \\omega_i}$ is the indicator function that returns 1 if $\\theta_i\\neq \\omega_i$, and 0 otherwise.\n",
    "\n",
    "For example, the Hamming distance between your answer and the correct answer is $d(\\theta, \\omega)=4$, because you only ranked *The Dark Knight* is correctly. \n",
    "\n",
    "Finally, let's suppose that the probability of giving a particular answer (expressed as $\\theta$) is modeled as\n",
    "$$ p(\\theta \\,|\\, \\omega, \\lambda) \\propto  e^{-\\lambda\\, d(\\theta,\\, \\omega)}.$$\n",
    "\n",
    "### Part A:\n",
    "\n",
    "Implement an Metropolis algorithm to produce sample guesses from 500 individuals, with various $\\lambda$ values, $\\lambda=0.2, 0.5, 1.0$. What are the top five possible guesses?\n",
    "\n",
    "### Part B:\n",
    "Compute the probability that *The Shawshank Redemption* is ranked as the top movie (ranked number 1) by the Metropolis algorithm sampler. Compare the resulting probabilities for the various different $\\lambda$ values. Summarize your findings.\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "Gonna use the same code for MH, with different targets and proposals and what not.\n",
    "\n",
    "Note that this proposal,\n",
    "$$ p(\\theta \\,|\\, \\omega, \\lambda) \\propto  e^{-\\lambda\\, d(\\theta,\\, \\omega)}.$$\n",
    "says that the closer (in edit distance) to the correct answer the more likely, more probably, that answer is. So this is a symmetric distribution that peaks at the correct answer.\n",
    "\n",
    "From the math, you can see right away that $\\lambda$ controls the \"peakiness\" like how concentrated the probability is around the correct answer (like how probable it is that you'd guess a super totally wrong one).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda =  0.2\n",
      "accept percent: 0.874\n",
      "top 5 guesses: ['[1 2 3 4 5]' '[3 2 1 4 5]' '[5 2 3 4 1]' '[1 2 5 4 3]' '[1 4 2 3 5]']\n",
      "frequencies: [20 12 11 11 10]\n",
      "\n",
      "\n",
      "\n",
      "lambda =  0.5\n",
      "accept percent: 0.658\n",
      "top 5 guesses: ['[1 2 3 4 5]' '[1 4 3 2 5]' '[1 3 2 4 5]' '[1 2 4 5 3]' '[1 2 5 3 4]']\n",
      "frequencies: [51 24 17 15 11]\n",
      "\n",
      "\n",
      "\n",
      "lambda =  1.0\n",
      "accept percent: 0.44\n",
      "top 5 guesses: ['[1 2 3 4 5]' '[2 1 3 4 5]' '[1 3 2 4 5]' '[1 2 3 5 4]' '[1 2 5 4 3]']\n",
      "frequencies: [77 39 31 18 17]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import collections\n",
    "\n",
    "def metropolis(p, qdraw, nsamp, xinit, burn):\n",
    "    samples=[]\n",
    "    x_prev = xinit\n",
    "    accepted=0\n",
    "    for i in range(nsamp):\n",
    "        x_star = qdraw(x_prev)\n",
    "        p_star = p(x_star)\n",
    "        p_prev = p(x_prev)\n",
    "        pdfratio = p_star/p_prev\n",
    "        if np.random.uniform() < min(1, pdfratio):\n",
    "            samples.append(np.array_str(x_star))\n",
    "            x_prev = x_star\n",
    "            accepted += 1\n",
    "        else:#we always get a sample\n",
    "            samples.append(np.array_str(x_prev))\n",
    "            \n",
    "    return samples[int(burn * nsamp):], accepted * 1. / len(samples)\n",
    "\n",
    "\n",
    "omega = np.array([1, 2, 3, 4, 5])\n",
    "proposal = lambda theta: np.random.permutation(theta)\n",
    "d = lambda theta: len((theta - omega)[(theta - omega) != 0])\n",
    "target_joint = lambda theta, l: np.e**(-l * d(theta))\n",
    "\n",
    "lambdas = [0.2, 0.5,1.0]\n",
    "\n",
    "for l in lambdas:\n",
    "    target = partial(target_joint, l=l)\n",
    "\n",
    "    x_init = np.array([2, 1, 3, 5, 4])\n",
    "    print 'lambda = ', l\n",
    "    num_samples = 500\n",
    "    samples, accept = metropolis(target, proposal, num_samples, x_init, burn=0)\n",
    "    print 'accept percent:', accept\n",
    "    vals, counts = np.unique(samples, return_counts=True)\n",
    "    top_5 = counts.argsort()[-5:][::-1]\n",
    "    print 'top 5 guesses:', vals[top_5]\n",
    "    print 'frequencies:', counts[top_5]\n",
    "    print '\\n\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the larger the lambda, the more likely you're to guess the right answer and the more tightly you're guesses will cluster around the right answer.\n",
    "\n",
    "To compute $p(\\text{shawshank is top} | lambda)$, we count how many guesses have 1 in the first entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda =  0.2\n",
      "top 5 guesses: ['[1 5 3 4 2]' '[1 2 3 4 5]' '[1 4 3 2 5]' '[1 2 3 5 4]' '[3 4 2 1 5]']\n",
      "frequencies: [15 14 12 10  9]\n",
      "p(shawshank ranked top | lambda):  0.276\n",
      "\n",
      "\n",
      "\n",
      "lambda =  0.5\n",
      "top 5 guesses: ['[1 3 2 4 5]' '[1 2 3 4 5]' '[1 4 3 2 5]' '[1 5 3 4 2]' '[3 2 1 4 5]']\n",
      "frequencies: [23 17 16 15 14]\n",
      "p(shawshank ranked top | lambda):  0.294\n",
      "\n",
      "\n",
      "\n",
      "lambda =  1.0\n",
      "top 5 guesses: ['[1 2 3 4 5]' '[3 2 1 4 5]' '[1 4 3 2 5]' '[5 2 3 4 1]' '[1 3 2 4 5]']\n",
      "frequencies: [138  36  26  25  19]\n",
      "p(shawshank ranked top | lambda):  0.528\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for l in lambdas:\n",
    "    target = partial(target_joint, l=l)\n",
    "\n",
    "    x_init = np.array([2, 1, 3, 5, 4])\n",
    "    print 'lambda = ', l\n",
    "    num_samples = 500\n",
    "    samples, accept = metropolis(target, proposal, num_samples, x_init, burn=0)\n",
    "    vals, counts = np.unique(samples, return_counts=True)\n",
    "    top_5 = counts.argsort()[-5:][::-1]\n",
    "    print 'top 5 guesses:', vals[top_5]\n",
    "    print 'frequencies:', counts[top_5]\n",
    "    print 'p(shawshank ranked top | lambda): ', len([s for s in samples if '[1 ' in s])  * 1. / len(samples)\n",
    "    print '\\n\\n'\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, $p(\\text{shawshank is top} | lambda)$ increases as lambda increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
