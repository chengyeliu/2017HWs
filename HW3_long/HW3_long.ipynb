{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Homework 3 \n",
    "\n",
    "## AMPTH 207: Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "\n",
    "**Due Date:** Thursday, Febrary 23rd, 2017 at 11:59pm\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers as well as your iPython notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Premise\n",
    "\n",
    "In supervised machine learning, a function that maps certain input data to a set of outputs is inferred from a labelled dataset called the training set, validated used a validation set, and the resulting learnt probabilities are then used to make predictions on unseen new examples of the dataset (the test set).\n",
    "\n",
    "The label of an element determines to which of a finite number of classes that element belongs to. Each element of the training is composed of two parts: a number $n$ of feature values, and the label corresponding to that element. *Learning* in this context is the process of inferring, given the set of features, what is the probability that a given example corresponds to a particular class. This is called a classification algorithm.\n",
    "\n",
    "In this homework, you will construct two different types of classifiers for the *MNIST* dataset, which dataset consists of 70,000 images of handwritten digits, each of which is 28x28 pixels. \n",
    "\n",
    "## Problem 1. Stochastic gradient descent for the logistic regresion\n",
    "\n",
    "One common classification algorithm is based on the logistic regression. In this part of the homework, we ask you to train a logistic regression classifier for the 10-dimensional space of the MNIST dataset.\n",
    "\n",
    "**Part 1: Preliminary questions:**\n",
    "\n",
    "* What are the features of the dataset?\n",
    "* What is a good strategy for splitting the dataset into training, validation and test sets?\n",
    "\n",
    "**Part 2: Gradient Descent**\n",
    "Using the softmax formulation, write a Theano expression graph that calculates the probability that a target element belongs to class $i$ (i.e., the probability that a given image represents a digit between 0 and 9), maximizes it over all classes, and computes the cost function using an L2 regularization approach (do no use the built-in theano softmax function, but rather write the function yourself). Minimize the resulting cost function using gradient descent only. How long does it take for your code to train with 50,000 training examples of the dataset?\n",
    "\n",
    "**Part 3: Stochastic Gradient Descent**\n",
    "Repeat the same exercise, but now train the algorithm using SGD with minibatches. \n",
    "\n",
    "* What is a good size of the minibatch?\n",
    "* What is the efficiency gain of the SGD algorithm with respect to GD only? \n",
    "\n",
    "Write the code in such a way that the performance of the classifier is evaluated after each iteration fo the training. Validate the classifier to avoid overfitting and to decide when to stop training. *Hint: *Plot the train and validation loss in the same plot. Finally, find a configuration that works well, i.e. a combination of number of minibatches, learning rate, regularization parameter, etc., that minimizes the loss. \n",
    "\n",
    "* What is the impact of the regularization parameter in your results?\n",
    "* Apply your classifier to the test set. What is the error you achieve with your classifier?\n",
    "\n",
    "\n",
    "\n",
    "## Problem 2. The Multilayer Perceptron (with one hidden layer)**\n",
    "The multilayer perceptron can be understood as a logistic regression classifier in which the input is first transformed using a learnt non-linear transformation. The non-linear transformation is usually chosen to be either the logistic function or the $\\tanh$ function, and its purpose is to project the data into a space where it becomes linealry separable The output of this so-called hidden layer is then passed to the Logistic Regression graph that we have constructed in the first part. In matrix notation:\n",
    "\n",
    "$$G(b^{(2)}+W^{(2)}(s(b^{(1)}W^{(1)}x)))$$\n",
    "\n",
    "with bias vectors $b^{(1)}$, $b^{(2)}$; weight matrices $W^{(1)}$, $W^{(2)}$ and activation functions $G$ and $s$.\n",
    "\n",
    "Using a similar architecture as in the first part and the same MNIST dataset, built a Theano graph for the multilayer perceptron, using both the logistic function and the *tanh* functions. Train and validate the perceptron using SGD and obtain a similar plot of the losses as in Problem 1. In particular, obtain the same plot for several values of the learning rate, the regularization parameter, and the number of minibatches. Discuss the results and select a combination of these parameters that decrease the test losses with respect to the logistic regression. How better can you get without running into overfitting problems?\n",
    "\n",
    "\n",
    "*Hint: *The initialization of the weights matrix for the hidden layer must assure that the units (neurons) of the perceptron operate in a regime where information gets propagated. For the $tanh$ function, it is advisable to initialize with the interval $[-\\sqrt{\\frac{6}{fan_{in}+fan_{out}}},\\sqrt{\\frac{6}{fan_{in}+fan_{out}}}]$, where $fan_{in}$ is the number of units in the $(i-1)$-th layer, and $fan_{out}$ is the number of units in the i-th layer. For the sigmoid function the interval should be $[-4\\sqrt{\\frac{6}{fan_{in}+fan_{out}}},4\\sqrt{\\frac{6}{fan_{in}+fan_{out}}}]$. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py36]",
   "language": "python",
   "name": "Python [py36]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
