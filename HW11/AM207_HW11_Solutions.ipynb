{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APMTH 207: Advanced Scientific Computing: \n",
    "## Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "## Homework #11\n",
    "**Harvard University**<br>\n",
    "**Spring 2017**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "**Due Date: ** Wednesday, April 26th 2017 at 11:59pm\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers as well as your iPython notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Homework #8 Revisited\n",
    "\n",
    "Recall the context of Homework #8: \n",
    "\n",
    "A plant nursery in Cambridge is exprimentally cross-breeding two types of hibiscus flowers: blue and pink. The goal is to create an exotic flower whose petals are pink with a ring of blue on each. \n",
    "\n",
    "There are four types of child plant that can result from this cross-breeding: \n",
    "\n",
    "  - Type 1: blue petals\n",
    "  - Type 2: pink petals \n",
    "  - Type 3: purple petals\n",
    "  - Type 4: pink petals with a blue ring on each (the desired effect). \n",
    "\n",
    "Out of 197 initial cross-breedings, the nursery obtained the following distribution over the four types of child plants: \n",
    "$$Y = (y_1, y_2, y_3, y_4) = (125, 18, 20, 34)$$\n",
    "where $y_i$ represents the number of child plants that are of type $i$.\n",
    "\n",
    "They know that the probability of obtaining each type of child plant in any single breeding experiment is as follows:\n",
    "$$ \\frac{\\theta+2}{4}, \\frac{1-\\theta}{4}, \\frac{1-\\theta}{4}, \\frac{\\theta}{4},$$\n",
    "where $\\theta$ is unknown.\n",
    "\n",
    "Sensibly, the nursery chose to model the observed data using a multinomial model; *they also imposed a prior on $\\theta$, $\\rm{Beta}(a, b)$*.\n",
    "\n",
    "Recall that to simplify sampling from their Bayesian model, the nursery augmented the data with a new variable $z$ such that:\n",
    "$$z + (y_1 - z) = y_1.$$\n",
    "That is, using $z$, they are breaking $y_1$, the number of type I child plants, into two subtypes. Let the probability of obtain the two subtype be $1/2$ and $\\theta/4$, respectively.\n",
    "\n",
    "In Homework 8, you implemented a Gibbs sampler for this Bayesian model to compute the posterior mean estimate of $\\theta$. \n",
    "\n",
    "In this homework we will investigate ways to compute the Maximum Likelihood Estimate (MLE) of $\\theta$.\n",
    "\n",
    "***Note:*** Expectation Maximization can also be applied to compute the posterior mode (MAP) estimates. We are choosing not to do that in this homework so that you are not just repeating the task from Homework #8.\n",
    "\n",
    "Parts A and B involve algebraic manipulations and no programming.\n",
    "\n",
    "### Part A:\n",
    "Treat the augmented model as a latent variable model. Write down an expression (up to unimportant constants - you must decide what unimportant means) for each of the following:\n",
    "\n",
    "1. the observed data log likelihood\n",
    "2. the complete data log likelihood\n",
    "\n",
    "**Hint:** You should already have the above from Homework #8.\n",
    "\n",
    "3. the Auxilary function, $Q(\\theta, \\theta^{(t-1)})$, or the expected complete data log likelihood, defined by\n",
    "$$Q(\\theta, \\theta^{(t-1)}) = \\mathbb{E}_{Z | Y, \\Theta^{t-1}}[\\text{the complete data log likelihood}]$$\n",
    "\n",
    "### Part B:\n",
    "We will maximize the likelihood through Expectation Maximization (EM). In order to preform EM, we must iterate through the following steps\n",
    "\n",
    "- (Expectation) Compute the Auxilary function, $Q(\\theta, \\theta^{t-1})$\n",
    "- (Maximization) Compute $\\theta^{t} = \\text{argmax}_\\theta Q(\\theta, \\theta^{(t-1)})$\n",
    "\n",
    "Thus, you must compute exact formulae for the following:\n",
    "1. the Auxilary function, $Q(\\theta, \\theta^{(t-1)})$, for a given $\\theta^{(t-1)}$. That is, compute the expectation of the complete data log likelihood.\n",
    "2. $\\theta^{t}$, by maximizing the Auxilary function $Q(\\theta, \\theta^{(t-1)})$.\n",
    "\n",
    "### Part C:\n",
    "Estimate the MLE of $\\theta$ using EM. Explain the advantage of treating this problem like a latent variable model and using EM to compute the MLE (i.e. why not compute MLE directly by maximizing the likelihood?)\n",
    "\n",
    "Compare this value with the posterior mean estimate of $\\theta$ from Homework #8. In general, what is the difference between MLE and MAP or posterior mean estimates of model parameters? That is, name a couple of major pro's and con's of each type of estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.66568914956\n",
      "1 0.631838674952\n",
      "2 0.627485219761\n",
      "3 0.626909582965\n",
      "4 0.626833192939\n",
      "5 0.626823050714\n",
      "6 0.626821704055\n",
      "7 0.626821525248\n",
      "8 0.626821501506\n",
      "9 0.626821498354\n",
      "10 0.626821497935\n",
      "0.626821497935\n"
     ]
    }
   ],
   "source": [
    "conv_thresh = 1e-9\n",
    "iters = 1000\n",
    "diff = 1.\n",
    "y = (125,18,20,34)\n",
    "thetas = [1.]\n",
    "i = 0\n",
    "\n",
    "while i < iters and diff > conv_thresh:\n",
    "    theta = ((y[0] * thetas[-1])/(2 + thetas[-1]) + y[3])/((y[0] * thetas[-1])/(2 + thetas[-1]) + y[3] + y[2] + y[1])\n",
    "    print i, theta\n",
    "    i += 1\n",
    "    diff = abs(theta - thetas[-1])\n",
    "    thetas.append(theta)\n",
    "    \n",
    "print thetas[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
